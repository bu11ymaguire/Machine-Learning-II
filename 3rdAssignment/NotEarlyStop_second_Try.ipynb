{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLZGeB9mse9u",
        "outputId": "73962653-0044-497f-fec1-4d28f4fb959a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your MLassignment3 folder\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(path_to_ml_assignment):\n",
        "    # List all files and directories in the specified path\n",
        "    files_in_folder = os.listdir(path_to_ml_assignment)\n",
        "    print(f\"Files in '{path_to_ml_assignment}':\")\n",
        "    for item in files_in_folder:\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"Error: The directory '{path_to_ml_assignment}' does not exist. Please check the path and ensure your Google Drive is mounted correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPrPQCwbsjdY",
        "outputId": "972f0663-6089-4ca4-de31-0ceb695c10f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in '/content/drive/MyDrive/MLassignment3':\n",
            "sample_submission.csv\n",
            "test.csv\n",
            "train.csv\n",
            "train\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to the train and test folders\n",
        "path_to_train_folder = os.path.join(path_to_ml_assignment, 'train')\n",
        "path_to_test_folder = os.path.join(path_to_ml_assignment, 'test')\n",
        "\n",
        "# Function to list files in a directory\n",
        "def list_files_in_directory(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        print(f\"\\nFiles in '{directory_path}':\")\n",
        "        files = os.listdir(directory_path)\n",
        "        # Print only the first few files to avoid overwhelming output\n",
        "        for i, item in enumerate(files):\n",
        "            if i < 10: # Displaying up to 10 files\n",
        "                print(item)\n",
        "            else:\n",
        "                print(f\"... and {len(files) - 10} more files.\")\n",
        "                break\n",
        "        if not files:\n",
        "            print(\"No files found.\")\n",
        "    else:\n",
        "        print(f\"Error: The directory '{directory_path}' does not exist.\")\n",
        "\n",
        "# List files in the train folder\n",
        "list_files_in_directory(path_to_train_folder)\n",
        "\n",
        "# List files in the test folder\n",
        "list_files_in_directory(path_to_test_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiHrQNFzstFG",
        "outputId": "a1888299-54a6-4ea7-b7b4-3fc711f85a16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files in '/content/drive/MyDrive/MLassignment3/train':\n",
            "train\n",
            "\n",
            "Files in '/content/drive/MyDrive/MLassignment3/test':\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Correct the path to the actual image directory within the train folder\n",
        "path_to_nested_train_images = os.path.join(path_to_train_folder, 'train')\n",
        "\n",
        "# List files in the corrected train image folder\n",
        "def list_files_in_directory_full(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        print(f\"\\nFiles in '{directory_path}':\")\n",
        "        files = os.listdir(directory_path)\n",
        "        if files:\n",
        "            for i, item in enumerate(files):\n",
        "                if i < 10: # Displaying up to 10 files\n",
        "                    print(item)\n",
        "                else:\n",
        "                    print(f\"... and {len(files) - 10} more files.\")\n",
        "                    break\n",
        "        else:\n",
        "            print(\"No files found.\")\n",
        "    else:\n",
        "        print(f\"Error: The directory '{directory_path}' does not exist. Please check the path.\")\n",
        "\n",
        "list_files_in_directory_full(path_to_nested_train_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC5olNnOsvJm",
        "outputId": "1298a126-fa83-4a3c-d076-72c009246be6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files in '/content/drive/MyDrive/MLassignment3/train/train':\n",
            "vqh.jpg\n",
            "vbp.jpg\n",
            "vxk.jpg\n",
            "vxm.jpg\n",
            "vju.jpg\n",
            "vtf.jpg\n",
            "vml.jpg\n",
            "vdf.jpg\n",
            "vsg.jpg\n",
            "vma.jpg\n",
            "... and 5301 more files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to your MLassignment3 folder (assuming it's already mounted)\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "\n",
        "# Define the path to train.csv\n",
        "train_csv_path = os.path.join(path_to_ml_assignment, 'train.csv')\n",
        "\n",
        "# Load the train.csv file\n",
        "try:\n",
        "    train_df = pd.read_csv(train_csv_path)\n",
        "    print(f\"'{train_csv_path}' file successfully loaded.\")\n",
        "    # Display the first few rows of the DataFrame\n",
        "    print(\"\\nDataFrame head:\")\n",
        "    display(train_df.head())\n",
        "\n",
        "    # Display basic information about the DataFrame\n",
        "    print(\"\\nDataFrame info:\")\n",
        "    train_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{train_csv_path}' not found. Please check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "q-u3kOSgsz0v",
        "outputId": "c88906d7-9185-48fd-ab52-fd8218b08e37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/MLassignment3/train.csv' file successfully loaded.\n",
            "\n",
            "DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    id            artist\n",
              "0  dxi   Diego Velazquez\n",
              "1  lxu  Vincent van Gogh\n",
              "2  edc      Claude Monet\n",
              "3  bis       Edgar Degas\n",
              "4  jly  Hieronymus Bosch"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b8db324-1e76-4ae1-a5b7-d5286edb078f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>artist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dxi</td>\n",
              "      <td>Diego Velazquez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lxu</td>\n",
              "      <td>Vincent van Gogh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>edc</td>\n",
              "      <td>Claude Monet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bis</td>\n",
              "      <td>Edgar Degas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jly</td>\n",
              "      <td>Hieronymus Bosch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b8db324-1e76-4ae1-a5b7-d5286edb078f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b8db324-1e76-4ae1-a5b7-d5286edb078f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b8db324-1e76-4ae1-a5b7-d5286edb078f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6eb2b40c-ffe2-42b6-b834-8896f0cd4d07\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6eb2b40c-ffe2-42b6-b834-8896f0cd4d07')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6eb2b40c-ffe2-42b6-b834-8896f0cd4d07 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"Error occurred while loading the file: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"lxu\",\n          \"jly\",\n          \"edc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Vincent van Gogh\",\n          \"Hieronymus Bosch\",\n          \"Claude Monet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5311 entries, 0 to 5310\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      5311 non-null   object\n",
            " 1   artist  5311 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 83.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "path_to_nested_train_images = os.path.join(path_to_ml_assignment, 'train', 'train')\n",
        "path_to_nested_test_images = os.path.join(path_to_ml_assignment, 'test', 'test')"
      ],
      "metadata": {
        "id": "w-dN0gPvs3NG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define a custom Dataset class to handle image loading and labeling from a DataFrame\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.img_labels = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.has_labels = 'artist' in dataframe.columns # Check if 'artist' column exists\n",
        "\n",
        "        if self.has_labels:\n",
        "            # For train data, create a mapping from artist names (labels) to integer indices\n",
        "            self.classes = sorted(self.img_labels['artist'].unique())\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        else:\n",
        "            # For test data without 'artist' column, we don't have true classes here\n",
        "            # We might populate self.classes from the train_dataset if needed for model output layer size\n",
        "            self.classes = [] # Placeholder\n",
        "            self.class_to_idx = {} # Placeholder\n",
        "\n",
        "        # Prepare a list of (image_full_path, label_index) tuples\n",
        "        self.samples = []\n",
        "        for idx, row in self.img_labels.iterrows():\n",
        "            img_name = row['id'] + '.jpg' # Assuming image IDs map to .jpg files\n",
        "            img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "            if os.path.exists(img_path):\n",
        "                label_idx = -1 # Use -1 or a specific dummy value for test set if no labels\n",
        "                if self.has_labels:\n",
        "                    label_name = row['artist']\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                self.samples.append((img_path, label_idx))\n",
        "            # else:\n",
        "            #     print(f\"Warning: Image file not found: {img_path}\") # Uncomment for debugging missing files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB') # Ensure 3 channels\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label # Return image and (potentially dummy) label\n",
        "\n"
      ],
      "metadata": {
        "id": "FrWhx0tWs9lb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#런타임 재활용시 avg, std 불러오기\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lATDmWgYtHP0",
        "outputId": "2209c6a1-9b11-4598-f71f-4650a95c3cca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjwkim628\u001b[0m (\u001b[33mjwkim628-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import wandb\n",
        "import json\n",
        "\n",
        "# 방법 B: Artifact 사용 (추천 - 가장 확실함)\n",
        "run = wandb.init(project=\"kaggle-artist-classification\", job_type=\"training\")\n",
        "artifact = run.use_artifact('dataset-stats:latest') # 최신 버전 가져오기\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "with open(os.path.join(artifact_dir, \"stats.json\"), \"r\") as f:\n",
        "    stats = json.load(f)\n",
        "\n",
        "calculated_mean = torch.tensor(stats['mean'])\n",
        "calculated_std = torch.tensor(stats['std'])\n",
        "\n",
        "print(f\"✅ Loaded stats from WandB: Mean={calculated_mean}, Std={calculated_std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "AciOTg5CtHw7",
        "outputId": "15471282-781f-4a9f-ff53-cfe80f1de088"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251123_125511-mxza7j4v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification/runs/mxza7j4v' target=\"_blank\">logical-firebrand-6</a></strong> to <a href='https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification' target=\"_blank\">https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification/runs/mxza7j4v' target=\"_blank\">https://wandb.ai/jwkim628-hanyang-university/kaggle-artist-classification/runs/mxza7j4v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded stats from WandB: Mean=tensor([0.5027, 0.4420, 0.3677]), Std=tensor([0.2103, 0.1947, 0.1761])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#When reallocate the runtime\n",
        "temp_transform_for_stats = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)), # Use consistent crop size\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 2. Create an instance of the custom dataset for calculating mean and std\n",
        "temp_train_dataset_for_stats = CustomImageDataset(\n",
        "    dataframe=train_df, # Use the `train_df` loaded from `train.csv`\n",
        "    img_dir=path_to_nested_train_images,\n",
        "    transform=temp_transform_for_stats\n",
        ")\n",
        "\n",
        "# 3. Create a DataLoader for the stats calculation\n",
        "temp_train_loader_for_stats = DataLoader(\n",
        "    temp_train_dataset_for_stats,\n",
        "    batch_size=32,\n",
        "    shuffle=False, # No need to shuffle for stats calculation\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# 4. Calculate mean and standard deviation\n",
        "print(\"Calculating mean and standard deviation for the training dataset...\")\n",
        "print(f\"Calculated Mean: {calculated_mean}\")\n",
        "print(f\"Calculated Std: {calculated_std}\")\n",
        "\n",
        "# 5. Define final transforms using the calculated mean and std\n",
        "# Transforms for training data (includes data augmentation)\n",
        "final_train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(), # Example augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "# Transforms for validation/test data (no augmentation, just consistent preprocessing)\n",
        "final_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "# 6. Load `test_df` for the test dataset (if not already loaded)\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "test_csv_path = os.path.join(path_to_ml_assignment, 'test.csv')\n",
        "\n",
        "try:\n",
        "    test_df = pd.read_csv(test_csv_path)\n",
        "    print(f\"'{test_csv_path}' file successfully loaded for test set.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{test_csv_path}' not found. Please check the path for test.csv.\")\n",
        "    # Handle case where test.csv might be missing, e.g., create a dummy dataframe\n",
        "    test_df = pd.DataFrame(columns=['id']) # Create df with 'id' column for test set\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred while loading test.csv: {e}\")\n",
        "\n",
        "\n",
        "# 7. Create actual train and test datasets using the CustomImageDataset\n",
        "train_dataset = CustomImageDataset(\n",
        "    dataframe=train_df,\n",
        "    img_dir=path_to_nested_train_images,\n",
        "    transform=final_train_transforms\n",
        ")\n",
        "\n",
        "# For the test_dataset, the CustomImageDataset will now handle the absence of the 'artist' column\n",
        "test_dataset = CustomImageDataset(\n",
        "    dataframe=test_df, # Use the loaded test_df which only has 'id'\n",
        "    img_dir=path_to_nested_test_images,\n",
        "    transform=final_test_transforms\n",
        ")\n",
        "\n",
        "# 8. Create final DataLoaders\n",
        "BATCH_SIZE = 32 # You can adjust this batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nTrain dataset size: {len(train_dataset)} images\")\n",
        "print(f\"Test dataset size: {len(test_dataset)} images\")\n",
        "print(f\"Number of classes (artists) in training data: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes (artists) in training data: {train_dataset.classes}\")\n",
        "print(f\"Train DataLoader has {len(train_loader)} batches.\")\n",
        "print(f\"Test DataLoader has {len(test_loader)} batches.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRxnPZfTtPVG",
        "outputId": "fd705d1b-0514-4287-c470-2a724a82aa01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating mean and standard deviation for the training dataset...\n",
            "Calculated Mean: tensor([0.5027, 0.4420, 0.3677])\n",
            "Calculated Std: tensor([0.2103, 0.1947, 0.1761])\n",
            "'/content/drive/MyDrive/MLassignment3/test.csv' file successfully loaded for test set.\n",
            "\n",
            "Train dataset size: 5311 images\n",
            "Test dataset size: 600 images\n",
            "Number of classes (artists) in training data: 50\n",
            "Classes (artists) in training data: ['Albrecht Du rer', 'Alfred Sisley', 'Amedeo Modigliani', 'Andrei Rublev', 'Andy Warhol', 'Camille Pissarro', 'Caravaggio', 'Claude Monet', 'Diego Rivera', 'Diego Velazquez', 'Edgar Degas', 'Edouard Manet', 'Edvard Munch', 'El Greco', 'Eugene Delacroix', 'Francisco Goya', 'Frida Kahlo', 'Georges Seurat', 'Giotto di Bondone', 'Gustav Klimt', 'Gustave Courbet', 'Henri Matisse', 'Henri Rousseau', 'Henri de Toulouse-Lautrec', 'Hieronymus Bosch', 'Jackson Pollock', 'Jan van Eyck', 'Joan Miro', 'Kazimir Malevich', 'Leonardo da Vinci', 'Marc Chagall', 'Michelangelo', 'Mikhail Vrubel', 'Pablo Picasso', 'Paul Cezanne', 'Paul Gauguin', 'Paul Klee', 'Peter Paul Rubens', 'Pierre-Auguste Renoir', 'Piet Mondrian', 'Pieter Bruegel', 'Raphael', 'Rembrandt', 'Rene Magritte', 'Salvador Dali', 'Sandro Botticelli', 'Titian', 'Vasiliy Kandinskiy', 'Vincent van Gogh', 'William Turner']\n",
            "Train DataLoader has 166 batches.\n",
            "Test DataLoader has 19 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: (B, C) - logits (model output before softmax)\n",
        "            targets: (B,) - class indices\n",
        "        \"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)  # probability of correct class\n",
        "\n",
        "        # Focal Loss = -alpha * (1 - pt)^gamma * ce_loss\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "uALuINlXtUlm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with skip connection\n",
        "        \"\"\"\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IKztqviqtW_9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=64,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        self.layer1 = self._make_layer(BasicBlock, 64, num_blocks=2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 128, num_blocks=2, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 256, num_blocks=2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 512, num_blocks=2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "\n",
        "        downsample = None\n",
        "\n",
        "\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    out_channels * block.expansion,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # He Initialization for Conv layers\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                # Batch Norm의 weight=1, bias=0으로 초기화\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initial Block\n",
        "        x = self.conv1(x)       # (B, 64, 112, 112)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)     # (B, 64, 56, 56)\n",
        "\n",
        "        # Residual Layers\n",
        "        x = self.layer1(x)      # (B, 64, 56, 56)\n",
        "        x = self.layer2(x)      # (B, 128, 28, 28)\n",
        "        x = self.layer3(x)      # (B, 256, 14, 14)\n",
        "        x = self.layer4(x)      # (B, 512, 7, 7)\n",
        "\n",
        "        # Classification Head\n",
        "        x = self.avgpool(x)     # (B, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1) # (B, 512)\n",
        "        x = self.fc(x)          # (B, num_classes)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8VlEWYIitZRl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet18(num_classes=num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainalbe_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainalbe_params:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1k8GvXotcDD",
        "outputId": "a63a7ac9-0a17-4492-ec73-123c4711c66e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 11,202,162\n",
            "Trainable Parameters: 11,202,162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_macro_f1(y_true, y_pred, num_classes):\n",
        "    f1s = []\n",
        "    for c in range(num_classes):\n",
        "        tp = ((y_pred==c)&(y_true==c)).sum().item()\n",
        "        fp = ((y_pred==c)&(y_true!=c)).sum().item()\n",
        "        fn = ((y_pred!=c)&(y_true==c)).sum().item()\n",
        "        precision = tp/(tp+fp) if tp+fp>0 else 0\n",
        "        recall   = tp/(tp+fn) if tp+fn>0 else 0\n",
        "        f1 = (2*precision*recall)/(precision+recall) if precision+recall>0 else 0\n",
        "        f1s.append(f1)\n",
        "    return sum(f1s)/len(f1s)"
      ],
      "metadata": {
        "id": "TgG3G01utegf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "\n",
        "seed = 42\n",
        "val_ratio = 0.1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "labels = torch.tensor([s[1] for s in train_dataset.samples], dtype=torch.long)\n",
        "\n",
        "unique_classes = torch.unique(labels)\n",
        "train_idx = torch.tensor([], dtype=torch.long)\n",
        "val_idx = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "for c in unique_classes:\n",
        "\n",
        "    c_idx = (labels == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    perm = torch.randperm(len(c_idx))\n",
        "\n",
        "    n_val = max(1, int(len(c_idx) * val_ratio))\n",
        "\n",
        "\n",
        "    val_idx = torch.cat([val_idx, c_idx[perm[:n_val]]])\n",
        "\n",
        "    train_idx = torch.cat([train_idx, c_idx[perm[n_val:]]])\n",
        "\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader_split = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Original Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Train Subset Size: {len(train_subset)}\")\n",
        "print(f\"Validation Subset Size: {len(val_subset)}\")\n",
        "print(f\"Train DataLoader (Split) has {len(train_loader_split)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBOvkm_tgwi",
        "outputId": "55dc8b02-60b7-407d-903b-a14b198d97a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Train Dataset Size: 5311\n",
            "Train Subset Size: 4801\n",
            "Validation Subset Size: 510\n",
            "Train DataLoader (Split) has 151 batches.\n",
            "Validation DataLoader has 16 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with Dropout/BatchNorm/Manual Residual Blocks\n",
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        # Residual layers\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels, 1, stride=stride, bias=False),\n",
        "                                       nn.BatchNorm2d(out_channels))\n",
        "        layers = [BasicBlock(self.in_channels, out_channels, stride, downsample)]\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
        "        x = self.avgpool(x); x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iiTC1ll1tj-Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomResNet18(num_classes=50).to(device)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ],
      "metadata": {
        "id": "UrXS4Vdotmew"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
      ],
      "metadata": {
        "id": "iFeltEpztpFS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Random Sampler\n",
        "labels = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "\n",
        "from collections import Counter\n",
        "class_counts = Counter(labels.tolist())\n",
        "\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, sampler=weighted_sampler, num_workers=0)\n",
        "\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "ofq0gjd4traR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 50\n",
        "model = CustomResNet18(num_classes=num_classes).to(device)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "num_epochs = 50\n",
        "\n",
        "best_macro_f1 = 0\n",
        "best_state_dict = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    trn_loss, trn_correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if torch.rand(1).item() < 0.25:\n",
        "            images, y_a, y_b, lam = mixup_data(images, labels)\n",
        "            outputs = model(images)\n",
        "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trn_loss += loss.item() * images.size(0)\n",
        "        trn_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    trn_acc = trn_correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    val_outputs, val_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            val_total += images.size(0)\n",
        "            val_outputs.append(outputs.argmax(1).cpu())\n",
        "            val_targets.append(labels.cpu())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_outputs = torch.cat(val_outputs)\n",
        "    val_targets = torch.cat(val_targets)\n",
        "    val_macro_f1 = custom_macro_f1(val_targets, val_outputs, num_classes=num_classes)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: train loss {trn_loss / total:.4f}, acc {trn_acc:.4f}; \"\n",
        "          f\"val loss {val_loss / val_total:.4f}, acc {val_acc:.4f}, macro F1 {val_macro_f1:.4f}\")\n",
        "\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        best_state_dict = model.state_dict()\n",
        "        print(f\"  -> Best Macro F1 updated: {best_macro_f1:.4f}\")\n",
        "\n",
        "if best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    print(f\"Loaded best model with Macro F1: {best_macro_f1:.4f}\")\n",
        "\n",
        "print(\"Finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "1KsHnEehtt19",
        "outputId": "cec8dff0-5789-498a-da97-aa6fd898a58b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1734559473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-247706294.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure 3 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60849284"
      },
      "source": [
        "# Task\n",
        "Optimize the DataLoaders by setting `num_workers` to leverage multiple CPU cores for faster image loading, then resume the model training for 50 epochs, tracking and reporting the best validation macro F1 score achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188033d3"
      },
      "source": [
        "## Optimize DataLoaders\n",
        "\n",
        "### Subtask:\n",
        "Modify the `DataLoader` creation for both the training and validation sets to utilize multiple worker processes (`num_workers`) for faster image loading and preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832e6cad"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to optimize the DataLoaders by utilizing multiple worker processes. The `os` module is already imported. I will modify the existing `DataLoader` definitions for `train_loader` and `val_loader` to set `num_workers` to `os.cpu_count()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "93116865",
        "outputId": "1bdd60e7-4620-49b3-d1c0-affb9868f919"
      },
      "source": [
        "import os\n",
        "\n",
        "# Weighted Random Sampler\n",
        "labels = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "\n",
        "from collections import Counter\n",
        "class_counts = Counter(labels.tolist())\n",
        "\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Modify num_workers to use all available CPU cores\n",
        "num_workers_value = os.cpu_count()\n",
        "print(f\"Using {num_workers_value} workers for DataLoader.\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, sampler=weighted_sampler, num_workers=num_workers_value)\n",
        "\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=num_workers_value)\n",
        "\n",
        "print(f\"Train DataLoader (with weighted sampler) has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1912992580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Weighted Random Sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f263f740"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'torch' is not defined` indicates that the `torch` module was not imported in the current code block. I need to add `import torch` at the beginning of the cell to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "05a8da1f",
        "outputId": "8f9fa1fd-ea14-463c-d9b6-4b1225134916"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# Weighted Random Sampler\n",
        "labels = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "\n",
        "class_counts = Counter(labels.tolist())\n",
        "\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Modify num_workers to use all available CPU cores\n",
        "num_workers_value = os.cpu_count()\n",
        "print(f\"Using {num_workers_value} workers for DataLoader.\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, sampler=weighted_sampler, num_workers=num_workers_value)\n",
        "\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=num_workers_value)\n",
        "\n",
        "print(f\"Train DataLoader (with weighted sampler) has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_subset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1976040576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Weighted Random Sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_subset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ad1a839"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'train_subset' is not defined` indicates that the `train_subset` and `val_subset` variables are not available in the current execution context. These were defined in a previous cell (`soBOvkm_tgwi`). I need to re-include the code that defines `train_subset` and `val_subset` before creating the DataLoaders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "998326ac",
        "outputId": "38cf24d5-c4b4-4fc4-a131-cf5a240cb0d1"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Re-create train_subset and val_subset, as they seem to be undefined\n",
        "# This code is adapted from cell 'soBOvkm_tgwi'\n",
        "seed = 42\n",
        "val_ratio = 0.1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Assuming train_dataset is still defined from a previous cell ('eRxnPZfTtPVG')\n",
        "# If not, it would need to be re-created as well.\n",
        "labels_full_dataset = torch.tensor([s[1] for s in train_dataset.samples], dtype=torch.long)\n",
        "\n",
        "unique_classes = torch.unique(labels_full_dataset)\n",
        "train_idx = torch.tensor([], dtype=torch.long)\n",
        "val_idx = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "for c in unique_classes:\n",
        "    c_idx = (labels_full_dataset == c).nonzero(as_tuple=True)[0]\n",
        "    perm = torch.randperm(len(c_idx))\n",
        "    n_val = max(1, int(len(c_idx) * val_ratio))\n",
        "    val_idx = torch.cat([val_idx, c_idx[perm[:n_val]]])\n",
        "    train_idx = torch.cat([train_idx, c_idx[perm[n_val:]]])\n",
        "\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "print(f\"Original Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Train Subset Size: {len(train_subset)}\")\n",
        "print(f\"Validation Subset Size: {len(val_subset)}\")\n",
        "\n",
        "# Weighted Random Sampler\n",
        "# Now using labels from train_subset\n",
        "labels_train_subset = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "class_counts = Counter(labels_train_subset.tolist())\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels_train_subset]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Modify num_workers to use all available CPU cores\n",
        "num_workers_value = os.cpu_count()\n",
        "print(f\"Using {num_workers_value} workers for DataLoader.\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, sampler=weighted_sampler, num_workers=num_workers_value)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=num_workers_value)\n",
        "\n",
        "print(f\"Train DataLoader (with weighted sampler) has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2436550696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Assuming train_dataset is still defined from a previous cell ('eRxnPZfTtPVG')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# If not, it would need to be re-created as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlabels_full_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0munique_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_full_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "409e0194"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'train_dataset' is not defined` indicates that the `train_dataset` variable, which is critical for creating `train_subset` and `val_subset`, was not available in the current execution context. This often happens if an earlier cell that defines `train_dataset` was not executed or its state was lost. To resolve this, I need to re-initialize `train_dataset`, `test_dataset`, and then re-create `train_subset` and `val_subset` before defining the DataLoaders with optimized `num_workers`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5f67c5ea",
        "outputId": "d3bdf98d-4b19-495d-a72c-ad5420aa82f3"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Re-define CustomImageDataset class (if not globally available due to runtime state)\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.img_labels = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.has_labels = 'artist' in dataframe.columns\n",
        "\n",
        "        if self.has_labels:\n",
        "            self.classes = sorted(self.img_labels['artist'].unique())\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        else:\n",
        "            self.classes = []\n",
        "            self.class_to_idx = {}\n",
        "\n",
        "        self.samples = []\n",
        "        for idx, row in self.img_labels.iterrows():\n",
        "            img_name = row['id'] + '.jpg'\n",
        "            img_path = os.path.join(self.img_dir, img_name)\n",
        "            if os.path.exists(img_path):\n",
        "                label_idx = -1\n",
        "                if self.has_labels:\n",
        "                    label_name = row['artist']\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                self.samples.append((img_path, label_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# --- Re-establish necessary variables from previous cells --- #\n",
        "\n",
        "# Path definitions (from w-dN0gPvs3NG)\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "path_to_nested_train_images = os.path.join(path_to_ml_assignment, 'train', 'train')\n",
        "path_to_nested_test_images = os.path.join(path_to_ml_assignment, 'test', 'test')\n",
        "\n",
        "# Load train_df (from q-u3kOSgsz0v)\n",
        "train_csv_path = os.path.join(path_to_ml_assignment, 'train.csv')\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Load test_df (from eRxnPZfTtPVG)\n",
        "test_csv_path = os.path.join(path_to_ml_assignment, 'test.csv')\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Load calculated_mean and calculated_std (from AciOTg5CtHw7 and eRxnPZfTtPVG)\n",
        "# Assuming 'calculated_mean' and 'calculated_std' were already loaded via wandb\n",
        "# If wandb.init() was interrupted, these might need to be reloaded.\n",
        "# For robustness, we can hardcode them if they were successfully printed in 'AciOTg5CtHw7'.\n",
        "# From the output of AciOTg5CtHw7:\n",
        "calculated_mean = torch.tensor([0.5027, 0.4420, 0.3677])\n",
        "calculated_std = torch.tensor([0.2103, 0.1947, 0.1761])\n",
        "\n",
        "# Define final transforms using the calculated mean and std (from eRxnPZfTtPVG)\n",
        "final_train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "final_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "# Create actual train and test datasets (from eRxnPZfTtPVG)\n",
        "train_dataset = CustomImageDataset(\n",
        "    dataframe=train_df,\n",
        "    img_dir=path_to_nested_train_images,\n",
        "    transform=final_train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = CustomImageDataset(\n",
        "    dataframe=test_df,\n",
        "    img_dir=path_to_nested_test_images,\n",
        "    transform=final_test_transforms\n",
        ")\n",
        "\n",
        "# --- Re-create train_subset and val_subset (from soBOvkm_tgwi) --- #\n",
        "seed = 42\n",
        "val_ratio = 0.1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "labels_full_dataset = torch.tensor([s[1] for s in train_dataset.samples], dtype=torch.long)\n",
        "unique_classes = torch.unique(labels_full_dataset)\n",
        "\n",
        "train_idx = torch.tensor([], dtype=torch.long)\n",
        "val_idx = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "for c in unique_classes:\n",
        "    c_idx = (labels_full_dataset == c).nonzero(as_tuple=True)[0]\n",
        "    perm = torch.randperm(len(c_idx))\n",
        "    n_val = max(1, int(len(c_idx) * val_ratio))\n",
        "    val_idx = torch.cat([val_idx, c_idx[perm[:n_val]]])\n",
        "    train_idx = torch.cat([train_idx, c_idx[perm[n_val:]]])\n",
        "\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "print(f\"Original Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Train Subset Size: {len(train_subset)}\")\n",
        "print(f\"Validation Subset Size: {len(val_subset)}\")\n",
        "\n",
        "# --- Optimize DataLoaders with num_workers --- #\n",
        "\n",
        "# Weighted Random Sampler\n",
        "# Now using labels from train_subset\n",
        "labels_train_subset = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "class_counts = Counter(labels_train_subset.tolist())\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels_train_subset]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Modify num_workers to use all available CPU cores\n",
        "num_workers_value = os.cpu_count()\n",
        "print(f\"Using {num_workers_value} workers for DataLoader.\")\n",
        "\n",
        "BATCH_SIZE = 32 # Assuming BATCH_SIZE is 32 as in eRxnPZfTtPVG\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, sampler=weighted_sampler, num_workers=num_workers_value, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_value, pin_memory=True)\n",
        "\n",
        "print(f\"Train DataLoader (with weighted sampler) has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/MLassignment3/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3211111265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Load train_df (from q-u3kOSgsz0v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrain_csv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_ml_assignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Load test_df (from eRxnPZfTtPVG)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MLassignment3/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fc556e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The `FileNotFoundError` for `train.csv` indicates that the file could not be accessed, most likely due to a transient issue with Google Drive mounting or access within the Colab environment. Although the drive was previously mounted and files were listed, re-running `drive.mount()` at the beginning of this comprehensive setup cell is the most robust way to ensure file system access before attempting to load data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10aa8ce7",
        "outputId": "a2fb0d7c-ee69-4e4b-8ce5-a61ab29dc3aa"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure Google Drive is mounted at the start of this block\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Re-define CustomImageDataset class (if not globally available due to runtime state)\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.img_labels = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.has_labels = 'artist' in dataframe.columns\n",
        "\n",
        "        if self.has_labels:\n",
        "            self.classes = sorted(self.img_labels['artist'].unique())\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        else:\n",
        "            self.classes = []\n",
        "            self.class_to_idx = {}\n",
        "\n",
        "        self.samples = []\n",
        "        for idx, row in self.img_labels.iterrows():\n",
        "            img_name = row['id'] + '.jpg'\n",
        "            img_path = os.path.join(self.img_dir, img_name)\n",
        "            if os.path.exists(img_path):\n",
        "                label_idx = -1\n",
        "                if self.has_labels:\n",
        "                    label_name = row['artist']\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                self.samples.append((img_path, label_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# --- Re-establish necessary variables from previous cells --- #\n",
        "\n",
        "# Path definitions (from w-dN0gPvs3NG)\n",
        "path_to_ml_assignment = '/content/drive/MyDrive/MLassignment3'\n",
        "path_to_nested_train_images = os.path.join(path_to_ml_assignment, 'train', 'train')\n",
        "path_to_nested_test_images = os.path.join(path_to_ml_assignment, 'test', 'test')\n",
        "\n",
        "# Load train_df (from q-u3kOSgsz0v)\n",
        "train_csv_path = os.path.join(path_to_ml_assignment, 'train.csv')\n",
        "\n",
        "# Add a check for file existence before reading\n",
        "if not os.path.exists(train_csv_path):\n",
        "    raise FileNotFoundError(f\"Error: '{train_csv_path}' not found. Please ensure Google Drive is mounted and the path is correct.\")\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Load test_df (from eRxnPZfTtPVG)\n",
        "test_csv_path = os.path.join(path_to_ml_assignment, 'test.csv')\n",
        "if not os.path.exists(test_csv_path):\n",
        "    raise FileNotFoundError(f\"Error: '{test_csv_path}' not found. Please ensure Google Drive is mounted and the path is correct.\")\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Load calculated_mean and calculated_std (from AciOTg5CtHw7 and eRxnPZfTtPVG)\n",
        "# These values are hardcoded as they were successfully computed and printed previously.\n",
        "calculated_mean = torch.tensor([0.5027, 0.4420, 0.3677])\n",
        "calculated_std = torch.tensor([0.2103, 0.1947, 0.1761])\n",
        "\n",
        "# Define final transforms using the calculated mean and std (from eRxnPZfTtPVG)\n",
        "final_train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "final_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=calculated_mean, std=calculated_std)\n",
        "])\n",
        "\n",
        "# Create actual train and test datasets (from eRxnPZfTtPVG)\n",
        "train_dataset = CustomImageDataset(\n",
        "    dataframe=train_df,\n",
        "    img_dir=path_to_nested_train_images,\n",
        "    transform=final_train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = CustomImageDataset(\n",
        "    dataframe=test_df,\n",
        "    img_dir=path_to_nested_test_images,\n",
        "    transform=final_test_transforms\n",
        ")\n",
        "\n",
        "# --- Re-create train_subset and val_subset (from soBOvkm_tgwi) --- #\n",
        "seed = 42\n",
        "val_ratio = 0.1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "labels_full_dataset = torch.tensor([s[1] for s in train_dataset.samples], dtype=torch.long)\n",
        "unique_classes = torch.unique(labels_full_dataset)\n",
        "\n",
        "train_idx = torch.tensor([], dtype=torch.long)\n",
        "val_idx = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "for c in unique_classes:\n",
        "    c_idx = (labels_full_dataset == c).nonzero(as_tuple=True)[0]\n",
        "    perm = torch.randperm(len(c_idx))\n",
        "    n_val = max(1, int(len(c_idx) * val_ratio))\n",
        "    val_idx = torch.cat([val_idx, c_idx[perm[:n_val]]])\n",
        "    train_idx = torch.cat([train_idx, c_idx[perm[n_val:]]])\n",
        "\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "print(f\"Original Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Train Subset Size: {len(train_subset)}\")\n",
        "print(f\"Validation Subset Size: {len(val_subset)}\")\n",
        "\n",
        "# --- Optimize DataLoaders with num_workers --- #\n",
        "\n",
        "# Weighted Random Sampler\n",
        "# Now using labels from train_subset\n",
        "labels_train_subset = torch.tensor([train_subset.dataset.samples[i][1] for i in train_subset.indices], dtype=torch.long)\n",
        "class_counts = Counter(labels_train_subset.tolist())\n",
        "sample_weights = [1.0 / class_counts[int(l)] for l in labels_train_subset]\n",
        "weighted_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Modify num_workers to use all available CPU cores\n",
        "num_workers_value = os.cpu_count()\n",
        "print(f\"Using {num_workers_value} workers for DataLoader.\")\n",
        "\n",
        "BATCH_SIZE = 32 # Assuming BATCH_SIZE is 32 as in eRxnPZfTtPVG\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, sampler=weighted_sampler, num_workers=num_workers_value, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_value, pin_memory=True)\n",
        "\n",
        "print(f\"Train DataLoader (with weighted sampler) has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Original Train Dataset Size: 5311\n",
            "Train Subset Size: 4801\n",
            "Validation Subset Size: 510\n",
            "Using 12 workers for DataLoader.\n",
            "Train DataLoader (with weighted sampler) has 151 batches.\n",
            "Validation DataLoader has 16 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbc040ea"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask of optimizing the DataLoaders by setting `num_workers` has been completed, as confirmed by the previous output. The next step is to resume the model training for 50 epochs using these optimized DataLoaders, tracking and reporting the best validation macro F1 score as per the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "a4d01289",
        "outputId": "59a687b2-e87a-49e3-f9f5-dcede5b75e32"
      },
      "source": [
        "import torch\n",
        "import wandb\n",
        "\n",
        "# Re-initialize model, criterion, optimizer, and scheduler to ensure a fresh start\n",
        "# and consistent state, as kernel state might have been lost or changed.\n",
        "# Assuming CustomResNet18, FocalLoss, device are already defined in earlier cells\n",
        "# If not, they would need to be re-imported/re-defined here.\n",
        "\n",
        "# Ensure device is correctly set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 50 # Assuming num_classes is 50 as established earlier\n",
        "\n",
        "model = CustomResNet18(num_classes=num_classes).to(device)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # T_max should match num_epochs\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "best_macro_f1 = 0\n",
        "best_state_dict = None\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    trn_loss, trn_correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if torch.rand(1).item() < 0.25:\n",
        "            images, y_a, y_b, lam = mixup_data(images, labels)\n",
        "            outputs = model(images)\n",
        "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trn_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # For training accuracy, calculate based on predicted class\n",
        "        _, predicted = outputs.max(1)\n",
        "        trn_correct += predicted.eq(labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    trn_acc = trn_correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    val_outputs, val_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # For validation accuracy and F1, calculate based on predicted class\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += images.size(0)\n",
        "            val_outputs.append(predicted.cpu())\n",
        "            val_targets.append(labels.cpu())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_outputs = torch.cat(val_outputs)\n",
        "    val_targets = torch.cat(val_targets)\n",
        "\n",
        "    # Assuming custom_macro_f1 is defined in a previous cell\n",
        "    val_macro_f1 = custom_macro_f1(val_targets, val_outputs, num_classes=num_classes)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: train loss {trn_loss / total:.4f}, acc {trn_acc:.4f}; \"\n",
        "          f\"val loss {val_loss / val_total:.4f}, acc {val_acc:.4f}, macro F1 {val_macro_f1:.4f}\")\n",
        "\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        best_state_dict = model.state_dict()\n",
        "        print(f\"  -> Best Macro F1 updated: {best_macro_f1:.4f}\")\n",
        "\n",
        "if best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    print(f\"Loaded best model with Macro F1: {best_macro_f1:.4f}\")\n",
        "\n",
        "print(\"Finished!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CustomResNet18' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4034970540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;31m# Assuming num_classes is 50 as established earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomResNet18' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e672efd2"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'CustomResNet18' is not defined` indicates that the model architecture and other essential components like `FocalLoss`, `mixup_data`, `mixup_criterion`, and `custom_macro_f1` were not defined in the current execution context. To resolve this, I will re-include the definitions of `FocalLoss`, `BasicBlock`, `CustomResNet18`, `custom_macro_f1`, `mixup_data`, and `mixup_criterion` in the same cell where the training loop is executed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8121223f",
        "outputId": "cad4a17f-9525-4f20-bd62-27671a1f6aa6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "# Re-define FocalLoss class\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Re-define BasicBlock class\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Re-define CustomResNet18 class\n",
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels, 1, stride=stride, bias=False),\n",
        "                                       nn.BatchNorm2d(out_channels))\n",
        "        layers = [BasicBlock(self.in_channels, out_channels, stride, downsample)]\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
        "        x = self.avgpool(x); x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Re-define custom_macro_f1 function\n",
        "def custom_macro_f1(y_true, y_pred, num_classes):\n",
        "    f1s = []\n",
        "    for c in range(num_classes):\n",
        "        tp = ((y_pred==c)&(y_true==c)).sum().item()\n",
        "        fp = ((y_pred==c)&(y_true!=c)).sum().item()\n",
        "        fn = ((y_pred!=c)&(y_true==c)).sum().item()\n",
        "        precision = tp/(tp+fp) if tp+fp>0 else 0\n",
        "        recall   = tp/(tp+fn) if tp+fn>0 else 0\n",
        "        f1 = (2*precision*recall)/(precision+recall) if precision+recall>0 else 0\n",
        "        f1s.append(f1)\n",
        "    return sum(f1s)/len(f1s)\n",
        "\n",
        "# Re-define mixup_data and mixup_criterion functions\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Ensure device is correctly set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 50 # Assuming num_classes is 50 as established earlier\n",
        "\n",
        "model = CustomResNet18(num_classes=num_classes).to(device)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # T_max should match num_epochs\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "best_macro_f1 = 0\n",
        "best_state_dict = None\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    trn_loss, trn_correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if torch.rand(1).item() < 0.25:\n",
        "            images, y_a, y_b, lam = mixup_data(images, labels)\n",
        "            outputs = model(images)\n",
        "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trn_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # For training accuracy, calculate based on predicted class\n",
        "        _, predicted = outputs.max(1)\n",
        "        trn_correct += predicted.eq(labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    trn_acc = trn_correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    val_outputs, val_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # For validation accuracy and F1, calculate based on predicted class\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += images.size(0)\n",
        "            val_outputs.append(predicted.cpu())\n",
        "            val_targets.append(labels.cpu())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_outputs = torch.cat(val_outputs)\n",
        "    val_targets = torch.cat(val_targets)\n",
        "\n",
        "    val_macro_f1 = custom_macro_f1(val_targets, val_outputs, num_classes=num_classes)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: train loss {trn_loss / total:.4f}, acc {trn_acc:.4f}; \"\n",
        "          f\"val loss {val_loss / val_total:.4f}, acc {val_acc:.4f}, macro F1 {val_macro_f1:.4f}\")\n",
        "\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        best_state_dict = model.state_dict()\n",
        "        print(f\"  -> Best Macro F1 updated: {best_macro_f1:.4f}\")\n",
        "\n",
        "if best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    print(f\"Loaded best model with Macro F1: {best_macro_f1:.4f}\")\n",
        "\n",
        "print(\"Finished!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 01: train loss 0.8585, acc 0.0779; val loss 0.9389, acc 0.0941, macro F1 0.0557\n",
            "  -> Best Macro F1 updated: 0.0557\n",
            "Epoch 02: train loss 0.7516, acc 0.1362; val loss 0.7812, acc 0.1196, macro F1 0.0838\n",
            "  -> Best Macro F1 updated: 0.0838\n",
            "Epoch 03: train loss 0.7102, acc 0.1602; val loss 0.7775, acc 0.1451, macro F1 0.1104\n",
            "  -> Best Macro F1 updated: 0.1104\n",
            "Epoch 04: train loss 0.6716, acc 0.1812; val loss 0.7503, acc 0.1255, macro F1 0.0976\n",
            "Epoch 05: train loss 0.6448, acc 0.1983; val loss 0.7208, acc 0.1647, macro F1 0.1379\n",
            "  -> Best Macro F1 updated: 0.1379\n",
            "Epoch 06: train loss 0.6106, acc 0.2233; val loss 0.7246, acc 0.1471, macro F1 0.1462\n",
            "  -> Best Macro F1 updated: 0.1462\n",
            "Epoch 07: train loss 0.5885, acc 0.2449; val loss 0.6866, acc 0.1608, macro F1 0.1392\n",
            "Epoch 08: train loss 0.5730, acc 0.2518; val loss 0.6343, acc 0.2471, macro F1 0.2050\n",
            "  -> Best Macro F1 updated: 0.2050\n",
            "Epoch 09: train loss 0.5194, acc 0.2889; val loss 0.6107, acc 0.2863, macro F1 0.2582\n",
            "  -> Best Macro F1 updated: 0.2582\n",
            "Epoch 10: train loss 0.5061, acc 0.2999; val loss 0.6302, acc 0.2549, macro F1 0.2275\n",
            "Epoch 11: train loss 0.4822, acc 0.3299; val loss 0.6203, acc 0.2490, macro F1 0.2129\n",
            "Epoch 12: train loss 0.4705, acc 0.3272; val loss 0.5779, acc 0.2843, macro F1 0.2582\n",
            "Epoch 13: train loss 0.4517, acc 0.3833; val loss 0.5918, acc 0.2882, macro F1 0.2467\n",
            "Epoch 14: train loss 0.4252, acc 0.4214; val loss 0.5735, acc 0.2980, macro F1 0.2531\n",
            "Epoch 15: train loss 0.3841, acc 0.4091; val loss 0.5082, acc 0.3549, macro F1 0.3004\n",
            "  -> Best Macro F1 updated: 0.3004\n",
            "Epoch 16: train loss 0.3762, acc 0.4549; val loss 0.6113, acc 0.2784, macro F1 0.2405\n",
            "Epoch 17: train loss 0.3428, acc 0.4839; val loss 0.5443, acc 0.3216, macro F1 0.2947\n",
            "Epoch 18: train loss 0.3125, acc 0.5166; val loss 0.5615, acc 0.2922, macro F1 0.2454\n",
            "Epoch 19: train loss 0.2918, acc 0.5101; val loss 0.5456, acc 0.3176, macro F1 0.2689\n",
            "Epoch 20: train loss 0.2482, acc 0.5797; val loss 0.5002, acc 0.3647, macro F1 0.3220\n",
            "  -> Best Macro F1 updated: 0.3220\n",
            "Epoch 21: train loss 0.2632, acc 0.5803; val loss 0.5412, acc 0.3353, macro F1 0.3117\n",
            "Epoch 22: train loss 0.2429, acc 0.6026; val loss 0.5036, acc 0.3510, macro F1 0.3185\n",
            "Epoch 23: train loss 0.2115, acc 0.6422; val loss 0.4825, acc 0.4039, macro F1 0.3656\n",
            "  -> Best Macro F1 updated: 0.3656\n",
            "Epoch 24: train loss 0.1921, acc 0.6463; val loss 0.4795, acc 0.3745, macro F1 0.3337\n",
            "Epoch 25: train loss 0.1626, acc 0.6755; val loss 0.4704, acc 0.4039, macro F1 0.3473\n",
            "Epoch 26: train loss 0.1588, acc 0.7030; val loss 0.5181, acc 0.3686, macro F1 0.3337\n",
            "Epoch 27: train loss 0.1505, acc 0.7113; val loss 0.4667, acc 0.4039, macro F1 0.3613\n",
            "Epoch 28: train loss 0.1480, acc 0.7001; val loss 0.4725, acc 0.4039, macro F1 0.3486\n",
            "Epoch 29: train loss 0.1135, acc 0.7650; val loss 0.4569, acc 0.4137, macro F1 0.3549\n",
            "Epoch 30: train loss 0.1202, acc 0.7609; val loss 0.4559, acc 0.4353, macro F1 0.3432\n",
            "Epoch 31: train loss 0.1093, acc 0.7778; val loss 0.4500, acc 0.4569, macro F1 0.3744\n",
            "  -> Best Macro F1 updated: 0.3744\n",
            "Epoch 32: train loss 0.1030, acc 0.8044; val loss 0.4352, acc 0.4451, macro F1 0.3812\n",
            "  -> Best Macro F1 updated: 0.3812\n",
            "Epoch 33: train loss 0.0765, acc 0.8467; val loss 0.4417, acc 0.4431, macro F1 0.3693\n",
            "Epoch 34: train loss 0.1035, acc 0.7675; val loss 0.4449, acc 0.4235, macro F1 0.3569\n",
            "Epoch 35: train loss 0.0684, acc 0.8250; val loss 0.4104, acc 0.4627, macro F1 0.4007\n",
            "  -> Best Macro F1 updated: 0.4007\n",
            "Epoch 36: train loss 0.1042, acc 0.7392; val loss 0.4491, acc 0.4725, macro F1 0.3963\n",
            "Epoch 37: train loss 0.0993, acc 0.8346; val loss 0.4248, acc 0.4863, macro F1 0.4011\n",
            "  -> Best Macro F1 updated: 0.4011\n",
            "Epoch 38: train loss 0.0602, acc 0.8607; val loss 0.4164, acc 0.4686, macro F1 0.3872\n",
            "Epoch 39: train loss 0.0648, acc 0.8432; val loss 0.4069, acc 0.4863, macro F1 0.3882\n",
            "Epoch 40: train loss 0.0534, acc 0.8661; val loss 0.4270, acc 0.4490, macro F1 0.3644\n",
            "Epoch 41: train loss 0.0782, acc 0.8305; val loss 0.4233, acc 0.4529, macro F1 0.3689\n",
            "Epoch 42: train loss 0.0543, acc 0.8734; val loss 0.4184, acc 0.4549, macro F1 0.3689\n",
            "Epoch 43: train loss 0.0639, acc 0.8800; val loss 0.4096, acc 0.4667, macro F1 0.3704\n",
            "Epoch 44: train loss 0.0748, acc 0.8142; val loss 0.4118, acc 0.4706, macro F1 0.3793\n",
            "Epoch 45: train loss 0.0780, acc 0.8373; val loss 0.4168, acc 0.4765, macro F1 0.3880\n",
            "Epoch 46: train loss 0.0596, acc 0.8471; val loss 0.4048, acc 0.4686, macro F1 0.3739\n",
            "Epoch 47: train loss 0.0920, acc 0.8327; val loss 0.4163, acc 0.4608, macro F1 0.3790\n",
            "Epoch 48: train loss 0.0812, acc 0.7971; val loss 0.4046, acc 0.4686, macro F1 0.3707\n",
            "Epoch 49: train loss 0.0779, acc 0.8375; val loss 0.4152, acc 0.4627, macro F1 0.3732\n",
            "Epoch 50: train loss 0.0731, acc 0.8402; val loss 0.4240, acc 0.4549, macro F1 0.3720\n",
            "Loaded best model with Macro F1: 0.4011\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90e11d5c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the results of the model training, including the best achieved validation macro F1 score and any other relevant performance metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1daf12c7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **DataLoader Optimization:** The DataLoaders were successfully optimized by setting `num_workers` to `os.cpu_count()` (which resulted in 12 workers) and enabling `pin_memory=True` for both training and validation sets, aiming to speed up image loading and preprocessing.\n",
        "*   **Model Training Duration:** The model was trained for a total of 50 epochs.\n",
        "*   **Best Validation Macro F1 Score:** The highest validation macro F1 score achieved during the 50-epoch training run was 0.4011.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The implementation of `num_workers` and `pin_memory` successfully optimized the data loading pipeline, which is crucial for efficient training, especially with large datasets.\n",
        "*   The macro F1 score of 0.4011 serves as a performance baseline for the current model and training configuration. Future efforts could focus on exploring advanced regularization techniques, different learning rate schedules, or more complex model architectures to further improve this metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# CustomImageDataset 클래스 재정의 (test set에서 img_id 반환하도록 수정)\n",
        "class CustomImageDataset_V2(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.img_labels = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.has_labels = 'artist' in dataframe.columns # 'artist' 컬럼 존재 여부 확인\n",
        "\n",
        "        if self.has_labels:\n",
        "            self.classes = sorted(self.img_labels['artist'].unique())\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        else:\n",
        "            self.classes = []\n",
        "            self.class_to_idx = {}\n",
        "\n",
        "        self.samples = []\n",
        "        for idx, row in self.img_labels.iterrows():\n",
        "            img_name = row['id'] + '.jpg'\n",
        "            img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "            if os.path.exists(img_path):\n",
        "                if self.has_labels:\n",
        "                    label_name = row['artist']\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    self.samples.append((img_path, label_idx)) # 학습/검증 시: 이미지 경로, 라벨 인덱스\n",
        "                else:\n",
        "                    self.samples.append((img_path, row['id'])) # 테스트 시: 이미지 경로, 원본 이미지 ID\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, target = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target # 이미지와 라벨 인덱스 또는 이미지 ID 반환\n",
        "\n",
        "print(\"CustomImageDataset_V2 정의 완료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eljqa4P87Yy3",
        "outputId": "faca54bc-d265-4eaa-874c-3607537a38bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomImageDataset_V2 정의 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 재정의된 CustomImageDataset_V2를 사용하여 test_dataset과 test_loader 재초기화\n",
        "\n",
        "# `final_test_transforms`와 `path_to_nested_test_images`는 이전 셀에서 정의된 것을 사용합니다.\n",
        "# `test_df`도 이전 셀에서 이미 로드되어 있다고 가정합니다.\n",
        "\n",
        "test_dataset_v2 = CustomImageDataset_V2(\n",
        "    dataframe=test_df,\n",
        "    img_dir=path_to_nested_test_images,\n",
        "    transform=final_test_transforms\n",
        ")\n",
        "\n",
        "# BATCH_SIZE와 num_workers_value는 이전 셀에서 정의된 것을 사용합니다.\n",
        "# test_loader_v2에 num_workers를 적용하여 이미지 로딩 속도 개선\n",
        "test_loader_v2 = DataLoader(test_dataset_v2, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_value, pin_memory=True)\n",
        "\n",
        "print(f\"Test dataset (V2) size: {len(test_dataset_v2)} images\")\n",
        "print(f\"Test DataLoader (V2) has {len(test_loader_v2)} batches.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cjdn1IX7p31",
        "outputId": "a63546e6-d02b-4f00-d612-2e737bed08dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset (V2) size: 600 images\n",
            "Test DataLoader (V2) has 19 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_img_ids = []\n",
        "all_pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_ids_batch in test_loader_v2:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "        all_pred_labels.extend(preds)\n",
        "        all_img_ids.extend(img_ids_batch)\n",
        "\n",
        "\n",
        "pred_artists = [train_dataset.classes[i] for i in all_pred_labels]\n",
        "\n",
        "submission = pd.DataFrame({'id': all_img_ids, 'artist': pred_artists})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission.csv 파일이 성공적으로 생성되었습니다!\")\n",
        "print(\"생성된 submission.csv의 첫 5행:\")\n",
        "display(submission.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "e3u_Ptu_7vGL",
        "outputId": "0cdf945e-c499-468c-e82b-4f73daf8d525"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission.csv 파일이 성공적으로 생성되었습니다!\n",
            "생성된 submission.csv의 첫 5행:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    id            artist\n",
              "0  pyz       Frida Kahlo\n",
              "1  vqn     Pablo Picasso\n",
              "2  cqk  Vincent van Gogh\n",
              "3  ubj    Francisco Goya\n",
              "4  tfs  Hieronymus Bosch"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8905bd49-4570-46ce-9b88-e2e8c289b230\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>artist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pyz</td>\n",
              "      <td>Frida Kahlo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vqn</td>\n",
              "      <td>Pablo Picasso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cqk</td>\n",
              "      <td>Vincent van Gogh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ubj</td>\n",
              "      <td>Francisco Goya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tfs</td>\n",
              "      <td>Hieronymus Bosch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8905bd49-4570-46ce-9b88-e2e8c289b230')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8905bd49-4570-46ce-9b88-e2e8c289b230 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8905bd49-4570-46ce-9b88-e2e8c289b230');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a037a5d0-fd7f-4ad2-acc1-44fd0afab9fb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a037a5d0-fd7f-4ad2-acc1-44fd0afab9fb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a037a5d0-fd7f-4ad2-acc1-44fd0afab9fb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(submission\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"vqn\",\n          \"tfs\",\n          \"cqk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Pablo Picasso\",\n          \"Hieronymus Bosch\",\n          \"Vincent van Gogh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "    files.download('submission.csv')\n",
        "    print(\"submission.csv 파일 다운로드가 시작되었습니다!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: submission.csv 파일을 찾을 수 없습니다. 파일이 올바르게 생성되었는지 확인해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"파일 다운로드 중 오류가 발생했습니다: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eP5p3NPP8NqS",
        "outputId": "24188e8d-a5bc-4003-eee5-0d6632b8955b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85ee5478-0e10-43eb-88c2-ac113b361e59\", \"submission.csv\", 11083)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv 파일 다운로드가 시작되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model's Filter Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "model_cpu = model.to('cpu')\n",
        "\n",
        "filters = model_cpu.conv1.weight.data.clone()\n",
        "\n",
        "num_filters = filters.shape[0]\n",
        "\n",
        "normalized_filters = torch.zeros_like(filters)\n",
        "for i in range(num_filters):\n",
        "    f = filters[i]\n",
        "    f_min = f.min()\n",
        "    f_max = f.max()\n",
        "    if f_max - f_min > 0:\n",
        "        normalized_filters[i] = (f - f_min) / (f_max - f_min)\n",
        "    else:\n",
        "        normalized_filters[i] = f - f_min\n",
        "\n",
        "rows = int(np.ceil(np.sqrt(num_filters)))\n",
        "cols = int(np.ceil(num_filters / rows))\n",
        "\n",
        "plt.figure(figsize=(cols*2, rows*2))\n",
        "\n",
        "for i in range(num_filters):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "    filter_to_show = normalized_filters[i].permute(1, 2, 0).numpy()\n",
        "    plt.imshow(filter_to_show)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Filters of the First Convolutional Layer (conv1)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2m7Z9rvi8RRY",
        "outputId": "34ad48ac-4478-4d48-e8a4-17105ddd1968"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 64 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomResNet18(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}