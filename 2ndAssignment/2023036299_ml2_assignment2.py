# -*- coding: utf-8 -*-
"""2023036299_ML2_Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JB4B1ezh5W695VJQyKl01hb_NM3kR1up
"""

import torch
import random
import torch.nn as nn

from torchvision import transforms, datasets

trainset = datasets.CIFAR10(root ='./ data ',train =True,download =True ,
transform = transforms.ToTensor())

testset = datasets.CIFAR10(root ='./ data ',train = False,download =True ,
transform = transforms.ToTensor())

SEED = 42

torch.manual_seed(SEED)
random.seed(SEED)

#1-(b)
Classes = trainset.classes

class_to_first_idx = {}
num_classes = 10

for idx, label in enumerate(trainset.targets):

  if not (label in class_to_first_idx):
      class_to_first_idx[label] = idx

  if len((class_to_first_idx)) == num_classes:
      break

for i in range(0,10):
  print(trainset[class_to_first_idx[i]])

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2,5, figsize=(12,6))
axes = axes.flatten()

for i in range(num_classes):
  data_index = class_to_first_idx[i]
  image_tensor, label = trainset[data_index]
  change_col = image_tensor.permute(1,2,0)
  ax = axes[i]
  ax.imshow(change_col)
  ax.set_title(Classes[label])
  ax.axis('off')

plt.tight_layout()
plt.show()

#1-(c)
print(len(trainset))

check_class = {}
for _, label in trainset:
  check_class[label] = check_class.get(label,0) + 1

print(check_class)

check = int(len(trainset) * 0.9)
Train = trainset[:check]
Val = trainset[check:]

from torch.utils.data import random_split

total_size = len(trainset)
train_size = int(total_size * 0.9)
val_size = total_size - train_size


Train, Val = random_split(trainset,[train_size,val_size],generator=torch.Generator().manual_seed(SEED))

print(len(Train))
print(len(Val))

check_train = {}

for _, label in Train:
  check_train[label] = check_train.get(label,0) + 1

print(check_train)

airplane_idx = 0
ship_idx = 8

def pick_class(Train,class_index):

  information = Train.dataset
  index_info = Train.indices

  pick_class = [
      idx for idx in index_info
      if information.targets[idx] == class_index
  ]

  return random.choice(pick_class)

random_airplane =Train.dataset[pick_class(Train,airplane_idx)][0].view(-1)
random_ship = Train.dataset[pick_class(Train,ship_idx)][0].view(-1)

W = (random_airplane - random_ship)
b = - torch.dot(W,(random_ship+random_airplane)/2)

image_for_size = trainset[0][0]
C,H, Width = image_for_size.shape

input_size = C * H * Width
output_size = 1

skeleton = nn.Linear(input_size,output_size)

with torch.no_grad(): #Initializaion
  skeleton.weight.data = W.unsqueeze(0)
  skeleton.bias.data = torch.tensor([b])

#Pick two classes Images from total Train, Val Set
from torch.utils.data import TensorDataset

svm_train_images = []
svm_train_labels = []

for idx in Train.indices:
  image, label = Train.dataset[idx]

  if label == airplane_idx:
    svm_train_images.append(image.view(-1))
    svm_train_labels.append(1.0)
  elif label == ship_idx:
    svm_train_images.append(image.view(-1))
    svm_train_labels.append(-1.0)

svm_train_images_tensor = torch.stack(svm_train_images)
svm_train_labels_tensor = torch.tensor(svm_train_labels).view(-1,1)
SVM_TRAIN = TensorDataset(svm_train_images_tensor, svm_train_labels_tensor)

svm_val_images = []
svm_val_labels = []

for idx in Val.indices:
  image, label = Val.dataset[idx]

  if label == airplane_idx:
    svm_val_images.append(image.view(-1))
    svm_val_labels.append(1.0)
  elif label == ship_idx:
    svm_val_images.append(image.view(-1))
    svm_val_labels.append(-1.0)

svm_val_images_tensor = torch.stack(svm_val_images)
svm_val_labels_tensor = torch.tensor(svm_val_labels).view(-1,1)
SVM_VAL = TensorDataset(svm_val_images_tensor, svm_val_labels_tensor)

#1-d
def Hinge_Loss(score,label,weight,Lambda):
  return torch.mean(torch.clamp(1-score*label,0)) + Lambda * weight.pow(2).sum() #.sqrt() -> sqrt is Monotonic increasing and when x = 0, cannot calculate derivative

import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

def train_evaluate_svm(model, svm_train_dataset, svm_val_dataset,
                         loss_fn, learning_rate, lambda_param,
                         num_epochs, batch_size, device):

    print(f"Hyperparameters: LR={learning_rate}, Lambda={lambda_param}, Epochs={num_epochs}, Batch={batch_size}")

    model = model.to(device)

    svm_train_loader = DataLoader(svm_train_dataset, batch_size=batch_size, shuffle=True)
    svm_val_loader = DataLoader(svm_val_dataset, batch_size=batch_size, shuffle=False)

    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    train_loss_history = []
    val_accuracy_history = []

    for epoch in range(num_epochs):

        model.train()
        running_loss = 0.0

        for images, labels in svm_train_loader:

            images = images.to(device)
            labels = labels.to(device)

            scores = model(images)

            loss = loss_fn(scores, labels, model.weight, lambda_param)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)

        epoch_train_loss = running_loss / len(svm_train_loader.dataset)
        train_loss_history.append(epoch_train_loss)

        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for val_images, val_labels in svm_val_loader:


                val_images = val_images.to(device)
                val_labels = val_labels.to(device)


                val_scores = model(val_images)

                predicted = torch.sign(val_scores)

                correct += (predicted == val_labels).sum().item()
                total += val_labels.size(0)

        epoch_val_accuracy = 100 * correct / total
        val_accuracy_history.append(epoch_val_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Accuracy: {epoch_val_accuracy:.2f}%")

    return train_loss_history, val_accuracy_history

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print({device})

#problem 1 - (e)
train_loss_64, val_64 = train_evaluate_svm(model=skeleton, svm_train_dataset=SVM_TRAIN, svm_val_dataset=SVM_VAL,
                         loss_fn=Hinge_Loss, learning_rate=0.001, lambda_param = 0.01,
                         num_epochs = 10, batch_size = 64, device = device)

#problem 1- (f)
SVM_TRAIN_MEAN = svm_train_images_tensor.mean(dim=0)
SVM_TRAIN_STD = svm_train_images_tensor.std(dim=0) + 1e-6

NOM_TRAIN_TENSOR = (svm_train_images_tensor - SVM_TRAIN_MEAN) / SVM_TRAIN_STD
NOM_VAL_TENSOR = (svm_val_images_tensor - SVM_TRAIN_MEAN) / SVM_TRAIN_STD

NORMAL_TRAIN_SET = TensorDataset(NOM_TRAIN_TENSOR, svm_train_labels_tensor)
NORMAL_VAL_SET = TensorDataset(NOM_VAL_TENSOR, svm_val_labels_tensor)

new_skeleton = nn.Linear(input_size,output_size)

with torch.no_grad(): #Initializaion
  new_skeleton.weight.data = W.unsqueeze(0)
  new_skeleton.bias.data = torch.tensor([b])

#problem 1- (g)
NOM_LOSS_64, NOM_VAL_64 = train_evaluate_svm(model=new_skeleton, svm_train_dataset=NORMAL_TRAIN_SET, svm_val_dataset=NORMAL_VAL_SET,
                         loss_fn=Hinge_Loss, learning_rate=0.001, lambda_param = 0.01,
                         num_epochs = 10, batch_size = 64, device = device)

#1 - (h), (i) : I will change batch_sizes, lambdas, learing_rates, and Normalization

batch_sizes = [64,128,256,512]
lambdas = [0.1, 0.01, 0.001, 0.0]
learning_rates = [0.0001,0.001, 0.01]

without_normalization = {}

for B in batch_sizes:
  for L in lambdas:
    for lr in learning_rates:
      skeleton_skratch = nn.Linear(input_size,output_size)
      with torch.no_grad(): #Initializaion
       skeleton_skratch.weight.data = W.unsqueeze(0)
       skeleton_skratch.bias.data = torch.tensor([b])
      result_loss, result_val = train_evaluate_svm(model=skeleton_skratch, svm_train_dataset=SVM_TRAIN, svm_val_dataset=SVM_VAL,
                         loss_fn=Hinge_Loss, learning_rate=lr, lambda_param = L,
                         num_epochs = 10, batch_size = B, device = device)
      without_normalization[(B, L, lr)] = result_val[-1]

best_parameter = max(without_normalization,key=without_normalization.get)
best_acc = without_normalization[best_parameter]

print(f"Best Accuracy: {best_acc:.2f}%")
print(f"Best Hyperparameters (Batch, Lambda, LR): {best_parameter}")

with_random = {}

for B in batch_sizes:
  for L in lambdas:
    for lr in learning_rates:
      skeleton_random = nn.Linear(input_size,output_size)
      # there is no with torch.no_grad()
      result_loss, result_val = train_evaluate_svm(model=skeleton_random, svm_train_dataset=SVM_TRAIN, svm_val_dataset=SVM_VAL,
                         loss_fn=Hinge_Loss, learning_rate=lr, lambda_param = L,
                         num_epochs = 10, batch_size = B, device = device)
      with_random[(B, L, lr)] = result_val[-1]

best_parameter = max(with_random,key=with_random.get)
best_acc = with_random[best_parameter]

print(f"Best Accuracy: {best_acc:.2f}%")
print(f"Best Hyperparameters (Batch, Lambda, LR): {best_parameter}")

with_random_normal = {}

for B in batch_sizes:
  for L in lambdas:
    for lr in learning_rates:
      skeleton_random = nn.Linear(input_size,output_size)
      # there is no with torch.no_grad()
      result_loss, result_val = train_evaluate_svm(model=skeleton_random, svm_train_dataset=NORMAL_TRAIN_SET, svm_val_dataset=NORMAL_VAL_SET,
                         loss_fn=Hinge_Loss, learning_rate=lr, lambda_param = L,
                         num_epochs = 10, batch_size = B, device = device)
      with_random_normal[(B, L, lr)] = result_val[-1]

best_parameter_normal = max(with_random_normal, key=with_random_normal.get)
best_acc_normal = with_random_normal[best_parameter_normal]

print(f"Best Accuracy: {best_acc_normal:.2f}%")
print(f"Best Hyperparameters (Batch, Lambda, LR): {best_parameter_normal}")

#1- (j) : Best Hyperparameters with Normalization (Batch, Lambda, LR): (64, 0.001, 0.001)
BEST_LR = 0.001
BEST_LAMDA = 0.001
BEST_BATCH = 64
EPOCHS = 10

svm_test_images = []
svm_test_labels = []

for i in range(len(testset.targets)):
    image, label = testset[i]

    if label == airplane_idx:
        svm_test_images.append(image.view(-1))
        svm_test_labels.append(1.0)
    elif label == ship_idx:
        svm_test_images.append(image.view(-1))
        svm_test_labels.append(-1.0)

svm_test_images_tensor = torch.stack(svm_test_images)
svm_test_labels_tensor = torch.tensor(svm_test_labels).view(-1, 1)

NOM_TEST_TENSOR = (svm_test_images_tensor - SVM_TRAIN_MEAN) / SVM_TRAIN_STD
NORMAL_TEST_SET = TensorDataset(NOM_TEST_TENSOR, svm_test_labels_tensor)

fianle_model = nn.Linear(input_size, output_size)

train_evaluate_svm(model=fianle_model,svm_train_dataset=NORMAL_TRAIN_SET,svm_val_dataset=NORMAL_VAL_SET,loss_fn=Hinge_Loss,learning_rate=BEST_LR,lambda_param=BEST_LAMDA,num_epochs=EPOCHS,batch_size=BEST_BATCH,device=device)

final_test_loader = DataLoader(NORMAL_TEST_SET,batch_size=BEST_BATCH, shuffle=False )

fianle_model.eval()
correct = 0
total = 0

with torch.no_grad():
  for test_images, test_labels in final_test_loader:
        test_images = test_images.to(device)
        test_labels = test_labels.to(device)

        test_scores = fianle_model(test_images)
        predicted = torch.sign(test_scores)

        correct += (predicted == test_labels).sum().item()
        total += test_labels.size(0)

final_Acc = 100 * correct / total

print(f"Final Test Accuracy: {final_Acc:.2f}%")

#2
'''
Since I confirmed that normalization performs well in step 1,
I first perform normalization and then define a function that classifies a specific idx and the remainder.
'''
all_train_images = []

for idx in Train.indices:
  all_train_images.append(Train.dataset[idx][0].view(-1))

all_train_tensor = torch.stack(all_train_images)

GLOBAL_MEAN = all_train_tensor.mean(dim=0)
GLOBAL_STD = all_train_tensor.std(dim=0) + 1e-6

def prepare_ova_dataset(target_class_idx, data, global_mean, global_std):
    images_list_norm = []
    labels_list_ova = []
    original_dataset = data.dataset

    for idx in data.indices:
        image, label = original_dataset[idx]
        image_flat = image.view(-1)
        image_norm = (image_flat - global_mean) / global_std
        images_list_norm.append(image_norm)

        if label == target_class_idx:
            labels_list_ova.append(1.0)
        else:
            labels_list_ova.append(-1.0)

    images_tensor = torch.stack(images_list_norm)
    labels_tensor = torch.tensor(labels_list_ova).view(-1, 1)

    return TensorDataset(images_tensor, labels_tensor)

fianl_best_parames_list = []
# 2- (a) & (b)
##1 showed the best results when batch size was 64. As part of optimization, batch_size was fixed to 64.
for index in range(0,10):
  print(f"\n===== Class {index} vs All Tuning Start =====")

  train__  = prepare_ova_dataset(index, Train, GLOBAL_MEAN, GLOBAL_STD)
  val__ = prepare_ova_dataset(index,Val,GLOBAL_MEAN, GLOBAL_STD )

  tuning_results = {}

  batch_size = 64
  lambdas = [0.1, 0.01, 0.001, 0.0]
  learning_rates = [0.001, 0.01]

  for L in lambdas:
    for lr in learning_rates:
      skeleton_random = nn.Linear(input_size, output_size)

      _, result_val = train_evaluate_svm(model=skeleton_random,svm_train_dataset=train__,
                                         svm_val_dataset=val__, loss_fn=Hinge_Loss, learning_rate=lr, lambda_param=L,num_epochs=10,
                                         batch_size=batch_size, device=device
                                         )
      tuning_results[(batch_size,L,lr)] = result_val[-1]

  best_params_for_class = max(tuning_results, key = tuning_results.get)
  best_acc_for_class = tuning_results[best_params_for_class]

  print(f"Class {index} Best Val Acc: {best_acc_for_class:.2f}%")
  print(f"Class {index} Best Params (B, L, lr): {best_params_for_class}")

  fianl_best_parames_list.append(best_params_for_class)

#testset Normalization

test_images_list_norm = []

for i in range(len(testset.targets)):
  image, label = testset[i]

  image_flat = image.view(-1)
  image_norm = (image_flat - GLOBAL_MEAN) / GLOBAL_STD
  test_images_list_norm.append(image_norm)

NOM_TEST_TENSOR = torch.stack(test_images_list_norm)
ORIGINAL_TEST_LABELS = torch.tensor(testset.targets)

NORMAL_TEST_SET_FINAL = TensorDataset(NOM_TEST_TENSOR, ORIGINAL_TEST_LABELS)

#2-(c)

classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

final_models_list = []

for index in range(10):

  B, L, lr = fianl_best_parames_list[index]

  train_ = prepare_ova_dataset(index, Train, GLOBAL_MEAN, GLOBAL_STD)
  val_ = prepare_ova_dataset(index, Val, GLOBAL_MEAN, GLOBAL_STD)

  final_model = nn.Linear(input_size, output_size)

  train_evaluate_svm(final_model, train_, val_, Hinge_Loss, lr, L, 10, B, device)

  final_models_list.append(final_model)

final_test_loader = DataLoader(NORMAL_TEST_SET_FINAL, batch_size = 64, shuffle = False)

class_correct = torch.zeros(10).to(device)
class_total = torch.zeros(10).to(device)

with torch.no_grad():
  for test_images, original_lables in final_test_loader:

    test_images = test_images.to(device)
    original_lables = original_lables.to(device)

    all_model_scores = []

    for model in final_models_list:
      model.eval()
      scores = model(test_images)
      all_model_scores.append(scores)

    all_scores_tensor = torch.cat(all_model_scores, dim = 1)
    predicted_labels = torch.argmax(all_scores_tensor, dim = 1)

    corrects_batch = (predicted_labels == original_lables)

    for i in range(10):
          label_mask = (original_lables == i)
          class_total[i] += label_mask.sum().item()
          class_correct[i] += (corrects_batch & label_mask).sum().item()

overall_accuracy = 100 * class_correct.sum().item() / class_total.sum().item()
print(overall_accuracy)

for i in range(10):
  acc = 100 * class_correct[i].item() / class_total[i].item()
  print(f"Accuracy of {classes[i]:>10s} : {acc:.2f} %")

#Let's test the ship and airplane classification for the 1vsall model.

model_airplane = final_models_list[airplane_idx].to(device).eval()
model_ship = final_models_list[ship_idx].to(device).eval()

airplane_mask = (ORIGINAL_TEST_LABELS == airplane_idx)
ship_mask = (ORIGINAL_TEST_LABELS == ship_idx)

airplane_images_only = NOM_TEST_TENSOR[airplane_mask]
airplane_labels_only = ORIGINAL_TEST_LABELS[airplane_mask]
airplane_test_set = TensorDataset(airplane_images_only, airplane_labels_only)

ship_images_only = NOM_TEST_TENSOR[ship_mask]
ship_labels_only = ORIGINAL_TEST_LABELS[ship_mask]
ship_test_set = TensorDataset(ship_images_only, ship_labels_only)

batch_size = 64
airplane_loader = DataLoader(airplane_test_set, batch_size=batch_size, shuffle=False)
ship_loader = DataLoader(ship_test_set, batch_size=batch_size, shuffle=False)

confused_as_airplane = 0
total_ships = 0

with torch.no_grad():
    for images, _ in ship_loader:
        images = images.to(device)

        scores = model_airplane(images)
        predicted = torch.sign(scores)


        confused_as_airplane += (predicted == 1.0).sum().item()
        total_ships += images.size(0)

confusion_rate_air = 100 * confused_as_airplane / total_ships

print(f"Total Ship: {total_ships}")
print(f"Confused_as_Airplane:{confused_as_airplane}")
print(f"(Confusion Rate_Air):{confusion_rate_air}")

confused_as_ship = 0
total_airplanes = 0

with torch.no_grad():
  for images, _ in airplane_loader:
      images = images.to(device)
      scores = model_ship(images)

      predicted = torch.sign(scores)

      confused_as_ship += (predicted == 1.0 ).sum().item()
      total_airplanes += images.size(0)

confusion_rate_ship = 100 * confused_as_ship / total_airplanes

print(f"Total Airplanes: {total_airplanes}")
print(f"Confused_as_ship:{confused_as_ship}")
print(f"(Confusion Rate_Ship):{confusion_rate_ship}")

#C : define softMax and CrossEntropy

def L_SCE(input, lables):
  find_max = torch.max(input, dim = 1, keepdim=True)[0]

  for_stable = input - find_max

  log_sum_exp = torch.log((torch.sum(torch.exp(for_stable), dim=1, keepdim=True))) + find_max

  label_change_dim = lables.unsqueeze(1)

  z_c = torch.gather(input, 1, label_change_dim)

  loss_per_sample = log_sum_exp - z_c

  return torch.mean(loss_per_sample)

def train_evaluate_softmax(model, train_dataset, val_dataset, loss_fn, learning_rate, num_epochs,
                           batch_size, device, lambda_param):
  model = model.to(device)

  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
  optimizer = optim.SGD(model.parameters(), lr=learning_rate)

  train_loss_history = []
  val_accuracy_history = []

  for epoch in range(num_epochs):

        model.train()
        running_loss = 0.0

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device)

            scores = model(images)

            loss = loss_fn(scores, labels)

            if lambda_param > 0:
                l2_loss = lambda_param * model.weight.pow(2).sum()
                loss += l2_loss


            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_loss_history.append(epoch_train_loss)

        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for val_images, val_labels in val_loader:
                val_images = val_images.to(device)
                val_labels = val_labels.to(device)

                val_scores = model(val_images)

                predicted = torch.argmax(val_scores, dim=1)

                correct += (predicted == val_labels).sum().item()
                total += val_labels.size(0)

        epoch_val_accuracy = 100 * correct / total
        val_accuracy_history.append(epoch_val_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Accuracy: {epoch_val_accuracy:.2f}%")

  return train_loss_history, val_accuracy_history

train_images_norm_list = []
train_labels_original_list = []

for idx in Train.indices:
    image, label = Train.dataset[idx]

    image_flat = image.view(-1)
    image_norm = (image_flat - GLOBAL_MEAN) / GLOBAL_STD
    train_images_norm_list.append(image_norm)

    train_labels_original_list.append(label)

SOFTMAX_TRAIN_TENSOR = torch.stack(train_images_norm_list)
SOFTMAX_TRAIN_LABELS = torch.tensor(train_labels_original_list, dtype=torch.long)

SOFTMAX_TRAIN_SET = TensorDataset(SOFTMAX_TRAIN_TENSOR, SOFTMAX_TRAIN_LABELS)

val_images_norm_list = []
val_labels_original_list = []

for idx in Val.indices:
    image, label = Val.dataset[idx]

    image_flat = image.view(-1)
    image_norm = (image_flat - GLOBAL_MEAN) / GLOBAL_STD
    val_images_norm_list.append(image_norm)

    val_labels_original_list.append(label)

SOFTMAX_VAL_TENSOR = torch.stack(val_images_norm_list)
SOFTMAX_VAL_LABELS = torch.tensor(val_labels_original_list, dtype=torch.long)

SOFTMAX_VAL_SET = TensorDataset(SOFTMAX_VAL_TENSOR, SOFTMAX_VAL_LABELS)

#3-(a)
model_sftmax = nn.Linear(input_size,10)

train_evaluate_softmax(model_sftmax, SOFTMAX_TRAIN_SET, SOFTMAX_VAL_SET, L_SCE, 0.001, 10, 64, device,0.001)

#3-(b)&c

batch_sizes = [64,128,256,512]
lambdas = [0.1, 0.01, 0.001, 0.0]
learning_rates = [0.0001, 0.001, 0.01]

softmax_tuning_results = {}

for B in batch_sizes :
  for L in lambdas:
    for lr in learning_rates:

      model_sft = nn.Linear(input_size,10)

      print(f"\n--- Testing: B={B}, L={L}, lr={lr} ---")

      _, val_acc_history = train_evaluate_softmax(
            model=model_sft,
            train_dataset=SOFTMAX_TRAIN_SET,
            val_dataset=SOFTMAX_VAL_SET,
            loss_fn=L_SCE,
            learning_rate=lr,
            num_epochs=10,
            batch_size=B,
            device=device,
            lambda_param=L
        )
      softmax_tuning_results[(B, L, lr)] = val_acc_history[-1]

best_params_softmax = max(softmax_tuning_results, key=softmax_tuning_results.get)
best_acc_softmax = softmax_tuning_results[best_params_softmax]

print(f"Best Accuracy: {best_acc_softmax:.2f}%")
print(f"Best Hyperparameters (Batch, Lambda, LR): {best_params_softmax}")

test_images_list_norm = []
test_labels_original_list = []

for i in range(len(testset.targets)):
  image, label = testset[i]

  image_flat = image.view(-1)
  image_norm = (image_flat - GLOBAL_MEAN) / GLOBAL_STD
  test_images_list_norm.append(image_norm)
  test_labels_original_list.append(label)

NOM_TEST_TENSOR = torch.stack(test_images_list_norm)
ORIGINAL_TEST_LABELS = torch.tensor(testset.targets, dtype = torch.long)

NORMAL_TEST_SET_FINAL = TensorDataset(NOM_TEST_TENSOR, ORIGINAL_TEST_LABELS)

BEST_BATCH, BEST_LAMBDA, BEST_LR = best_params_softmax

NUM_EPOCHS = 10

final_model_sftx = nn.Linear(input_size, 10)

train_evaluate_softmax(final_model_sftx, SOFTMAX_TRAIN_SET, SOFTMAX_VAL_SET, L_SCE, BEST_LR, NUM_EPOCHS, BEST_BATCH, device, BEST_LAMBDA)

final_test_loader = DataLoader(NORMAL_TEST_SET_FINAL, batch_size=BEST_BATCH, shuffle=False)
final_model_sftx.eval()

class_correct_scratch = torch.zeros(10).to(device)
class_total_scratch = torch.zeros(10).to(device)
classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

with torch.no_grad():
    for test_images, original_labels in final_test_loader:
        test_images = test_images.to(device)
        original_labels = original_labels.to(device)

        test_scores = final_model_sftx(test_images)
        predicted_labels = torch.argmax(test_scores, dim=1)

        corrects_batch = (predicted_labels == original_labels)
        for i in range(10):
            label_mask = (original_labels == i)
            class_total_scratch[i] += label_mask.sum().item()
            class_correct_scratch[i] += (corrects_batch & label_mask).sum().item()

overall_acc_scratch = 100 * class_correct_scratch.sum().item() / class_total_scratch.sum().item()
print(f"========================================")
print(f"(c) Your Scratch Model - Test Accuracy (Overall): {overall_acc_scratch:.2f}%")
print(f"========================================")
for i in range(10):
    if class_total_scratch[i] > 0:
        acc = 100 * class_correct_scratch[i].item() / class_total_scratch[i].item()
        print(f"Accuracy of {classes[i]:>10s} : {acc:.2f} %")

#3- (d)
loss_fn_pytorch = nn.CrossEntropyLoss()

model_pytorch = nn.Linear(input_size, 10).to(device)
optimizer_pytorch = optim.SGD(model_pytorch.parameters(), lr=BEST_LR)

train_loader_pytorch = DataLoader(SOFTMAX_TRAIN_SET, batch_size=BEST_BATCH, shuffle=True)

for epoch in range(NUM_EPOCHS):
    model_pytorch.train()
    running_loss = 0.0
    for images, labels in train_loader_pytorch:
        images = images.to(device)
        labels = labels.to(device)

        scores = model_pytorch(images)

        loss = loss_fn_pytorch(scores, labels)

        if BEST_LAMBDA > 0:
            l2_loss = BEST_LAMBDA * model_pytorch.weight.pow(2).sum()
            loss += l2_loss

        optimizer_pytorch.zero_grad()
        loss.backward()
        optimizer_pytorch.step()
        running_loss += loss.item() * images.size(0)

    epoch_loss = running_loss / len(train_loader_pytorch.dataset)
    print(f"Epoch {epoch+1}/{NUM_EPOCHS}, PyTorch Model Train Loss: {epoch_loss:.4f}")

print("\n--- (d) 2. Final Evaluation (PyTorch Model) ---")

model_pytorch.eval()

class_correct_pytorch = torch.zeros(10).to(device)
class_total_pytorch = torch.zeros(10).to(device)

with torch.no_grad():
    for test_images, original_labels in final_test_loader:
        test_images = test_images.to(device)
        original_labels = original_labels.to(device)

        test_scores = model_pytorch(test_images)
        predicted_labels = torch.argmax(test_scores, dim=1)

        corrects_batch = (predicted_labels == original_labels)
        for i in range(10):
            label_mask = (original_labels == i)
            class_total_pytorch[i] += label_mask.sum().item()
            class_correct_pytorch[i] += (corrects_batch & label_mask).sum().item()


print("\n--- Final Test Results (PyTorch Model) ---")

overall_acc_pytorch = 100 * class_correct_pytorch.sum().item() / class_total_pytorch.sum().item()
print(f"========================================")
print(f"(d) PyTorch Model - Test Accuracy (Overall): {overall_acc_pytorch:.2f}%")
print(f"========================================")
print("\n--- Per-Class Accuracy (PyTorch Model) ---")
for i in range(10):
    if class_total_pytorch[i] > 0:
        acc = 100 * class_correct_pytorch[i].item() / class_total_pytorch[i].item()
        print(f"Accuracy of {classes[i]:>10s} : {acc:.2f} %")
    else:
        print(f"Accuracy of {classes[i]:>10s} : N/A (No samples)")

print("\n--- Final Comparison ---")
print(f"(c) Your Scratch Model: {overall_acc_scratch:.2f}%")
print(f"(d) PyTorch Library Model: {overall_acc_pytorch:.2f}%")

#If the model I created is really similar to the built-in model, the hyperparameter tuning results for the built-in model will be similar.
batch_sizes = [64, 128, 256, 512]
lambdas = [0.1, 0.01, 0.001, 0.0]
learning_rates = [0.0001, 0.001, 0.01]

pytorch_tuning_results = {}

print("--- PyTorch Library (nn.CrossEntropyLoss) Hyperparameter Tuning Start ---")

for B in batch_sizes:
  for L in lambdas:
    for lr in learning_rates:

        model_pytorch = nn.Linear(input_size, 10)

        print(f"\n--- Testing PyTorch: B={B}, L={L}, lr={lr} ---")


        _, val_acc_history = train_evaluate_softmax(
            model=model_pytorch,
            train_dataset=SOFTMAX_TRAIN_SET,
            val_dataset=SOFTMAX_VAL_SET,
            loss_fn=loss_fn_pytorch,
            learning_rate=lr,
            num_epochs=10,
            batch_size=B,
            device=device,
            lambda_param=L
        )

        pytorch_tuning_results[(B, L, lr)] = val_acc_history[-1]

print("\n--- PyTorch Tuning Finished ---")

best_params_pytorch = max(pytorch_tuning_results, key=pytorch_tuning_results.get)
best_acc_pytorch = pytorch_tuning_results[best_params_pytorch]

print(f"\n--- (d) PyTorch Model - Best Results ---")
print(f"Best Accuracy: {best_acc_pytorch:.2f}%")
print(f"Best Hyperparameters (Batch, Lambda, LR): {best_params_pytorch}")

print("\n--- Final Comparison of Best Hyperparameters ---")
print(f"(c) Your Scratch Model:")
print(f"    Best Acc: {best_acc_softmax:.2f}%, Params: {best_params_softmax}")
print(f"(d) PyTorch Library Model:")
print(f"    Best Acc: {best_acc_pytorch:.2f}%, Params: {best_params_pytorch}")