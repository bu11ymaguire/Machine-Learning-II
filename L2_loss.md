## L2 손실(L2 Loss)에 대한 깊은 이해: 두 가지 관점

머신러닝, 특히 회귀 문제에서 L2 손실(MSE, Mean Squared Error)을 사용하는 이유는 크게 **실용적(경험적) 이유**와 **이론적 이유** 두 가지로 설명할 수 있습니다.

### 1. 실용적 관점: 최적화의 용이성 (선형대수학적 해석)

L2 손실은 수학적으로 다루기 좋은 **미분 가능한 볼록 함수(convex function)**입니다. 따라서 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘을 통해 유일한 최적해(global minimum)를 안정적으로 찾을 수 있습니다.

특히 선형 회귀의 경우, L2 손실을 최소화하는 해는 **정규방정식(Normal Equation)**이라는 행렬식으로 한 번에 계산할 수 있는 닫힌 형식의 해(closed-form solution)가 존재합니다.

$$w = (X^T X)^{-1} X^T y$$

이는 예측값 벡터($\mathbf{Xw}$)와 실제값 벡터($\mathbf{y}$) 사이의 **기하학적 거리(유클리드 거리)를 최소화**하는 해를 찾는 것과 같습니다.

---

### 2. 이론적 관점: 사전 지식의 인코딩 (확률론적 해석)

L2 손실을 선택하는 것은 단순히 계산이 편하기 때문만은 아닙니다. 이는 데이터 생성 과정에 대한 깊은 **통계적 사전 지식(prior knowledge)을 모델에 인코딩하는 행위**입니다.

#### 핵심 사전 지식: 오차는 가우시안 분포를 따른다

가장 핵심적인 사전 지식은 **"모델의 오차(noise)가 평균이 0인 가우시안(정규) 분포를 따른다"**는 가정입니다. 이를 바탕으로 다음과 같은 확률 모델을 세울 수 있습니다.

* **모델**: 실제 값 $y_i$는 예측값 $f(x_i)$에 평균이 0인 가우시안 오차 $\epsilon_i$가 더해진 것이다.
    $$y_i = f(x_i) + \epsilon_i \quad \text{where} \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)$$

* **결론**: 이 모델은 **"실제 값 $y_i$는 예측값 $f(x_i)$를 평균으로 하는 가우시안 분포에서 추출된 확률 변수다"**라는 의미가 됩니다.
    $$y_i \sim \mathcal{N}(f(x_i), \sigma^2)$$



이 가정 하나에 아래와 같은 여러 믿음이 인코딩되어 있습니다.
* **오차의 평균은 0이다**: 모델의 예측은 체계적으로 편향되지 않는다.
* **작은 오차가 더 흔하다**: 예측값에서 크게 벗어나는 경우는 드물다 (가우시안 분포의 종 모양).
* **오차의 분산은 일정하다**: 노이즈의 수준은 데이터 전체에 걸쳐 비슷하다.

#### 이론과 손실 함수의 연결: 최대우도추정 (MLE)

이 확률적 가정을 실제 손실 함수와 연결하는 원리가 바로 **최대우도추정(Maximum Likelihood Estimation, MLE)**입니다.

1.  **"Fitness" of $\theta$**: 모델 파라미터 $\theta$가 데이터를 얼마나 잘 설명하는지에 대한 '적합도'를 **가능도(Likelihood)**로 측정합니다. `likelihood(θ; data) = P_θ(data)`
2.  **MLE의 목표**: 우리가 가진 전체 데이터가 나타날 가능도(Total Likelihood)를 최대로 만드는 파라미터 $\theta$를 찾습니다.
3.  **수학적 전개**: 전체 가능도는 각 데이터의 가능도를 모두 곱한 값(`Π P_θ`)입니다. 이를 최적화하기 쉬운 **음의 로그 가능도(Negative Log-Likelihood, NLL)** 문제로 변환합니다.
4.  **결론**: "오차가 가우시안 분포를 따른다"는 가정하에 NLL을 최소화하는 식을 전개하면, 그 결과는 정확히 **오차 제곱의 합(Sum of Squared Errors)을 최소화**하는 것과 같아집니다.

$$\underset{w}{\text{argmin}} \sum_{i=1}^{n} (y_i - f_w(x_i))^2$$

---

### 최종 요약: 두 관점의 통합

결론적으로, L2 손실을 사용하여 최적의 $w$를 찾는 것은 두 가지 강력한 관점에서 모두 정당화됩니다.

| 관점 | **선형대수학 (기하학)** | **확률론 (통계학)** |
| :--- | :--- | :--- |
| **핵심 질문** | "데이터와 가장 가까운 선은 무엇인가?" | "데이터를 가장 잘 설명하는 모델은 무엇인가?" |
| **해석** | 벡터 간의 **거리** 최소화 | 데이터의 **가능도(Likelihood)** 최대화 |
| **언어** | 벡터, 공간, 정사영(Projection) | 확률, 분포, 추정 |
| **결과** | **최소제곱해 (Least Squares Solution)** | **최대우도추정해 (MLE Solution)** |

이처럼 서로 다른 두 지적 프레임워크가 **L2 손실 최소화**라는 동일한 결론에 도달한다는 사실이 L2 손실이 머신러닝에서 기본적이면서도 강력한 도구로 사용되는 이유입니다.
