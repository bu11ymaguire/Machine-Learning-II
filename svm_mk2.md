# SVM 심화 원리 및 확장 (Advanced SVM Concepts)

---

## 1. 일반화와 과적합 방지 (Generalization & Overfitting)

SVM은 단순히 훈련 데이터를 잘 나누는 것을 넘어, 새로운 데이터에 대한 일반화 성능을 높이는 것을 목표로 합니다.

### 왜 큰 마진(Large Margin)과 작은 가설 집합(Small Hypothesis Class)을 선택할까?

* **경험적 손실 vs 기대 손실**: 훈련 데이터에서의 오차(경험적 손실)를 줄이는 것뿐만 아니라, 미래 데이터에 대한 오차(기대 손실)를 줄이는 것이 최종 목표입니다. 이 둘의 차이를 **일반화 오차**라고 합니다.
* **큰 마진 가설**: 마진이 클수록 결정 경계가 데이터로부터 멀리 떨어져 있어, 노이즈에 덜 민감하고 높은 신뢰도(Confidence)를 가집니다.
* **작은 가설 집합**: 모델이 선택할 수 있는 함수의 복잡도를 제한하여, 훈련 데이터의 사소한 특징에 얽매이지 않는 단순하고 일반적인 규칙을 찾도록 유도합니다.

> **결론**: **마진을 최대화하는 것**은 수학적으로 `||w||`를 작게 제한하는 것과 같으며, 이는 곧 **가설 집합의 복잡도를 줄이는** 행위입니다. 이를 통해 SVM은 **신뢰도 상승**과 **복잡도 감소**라는 두 마리 토끼를 잡아 과적합을 방지합니다.

### 규제(Regularization)와 편향-분산 트레이드오프

* **규제의 역할**: 모델에 "가중치는 작아야 한다"와 같은 사전 지식(제약)을 주입하여, 훈련 데이터의 노이즈까지 학습하려는 과적합 경향을 막습니다.
* **편향-분산 트레이드오프**: 규제는 의도적으로 약간의 **편향(Bias)**을 추가하는 대신, 과적합의 주원인인 **분산(Variance)을 크게 낮춥니다.**

> **요약**: 약간의 규제(Regularization)를 통해 분산을 낮춰서 과적합을 줄인다.

---

## 2. 소프트 마진 SVM (Soft-Margin SVM)

실제 데이터는 노이즈 등으로 인해 완벽하게 선형으로 분리되지 않는 경우가 많습니다.

### "선허용 후관리" 철학

하드 마진 SVM은 단 하나의 예외도 허용하지 않지만, 소프트 마진 SVM은 약간의 오류를 허용하되 그에 대한 벌점을 부과하는 유연한 전략을 사용합니다.

* **슬랙 변수 (Slack Variable) `ξᵢ`**: 각 데이터(`i`)가 마진을 얼마나 위반했는지 나타내는 '허용치'입니다.
    * `ξᵢ = 0`: 마진 규칙을 잘 지킨 경우 (정상)
    * `0 < ξᵢ ≤ 1`: 마진 영역 안쪽에 위치한 경우 (규칙 위반)
    * `ξᵢ > 1`: 아예 잘못 분류된 경우 (오분류)

### 제약조건의 변화

슬랙 변수의 도입으로 제약조건이 유연하게 바뀝니다.

* **하드 마진**: $y^{(i)}(\mathbf{w}^T\mathbf{x}^{(i)} + w_0) \geq 1$ (절대 규칙)
* **소프트 마진**: $y^{(i)}(\mathbf{w}^T\mathbf{x}^{(i)} + w_0) \geq 1 - \xi_i$ (유연한 규칙)

## 3. 목적 함수와 하이퍼파라미터 `γ`

소프트 마진 SVM의 목적 함수는 **마진 최대화**와 **오류 최소화** 사이의 균형을 맞추는 것입니다.

$$
\min_{\mathbf{w},w_0,\xi} \frac{\|\mathbf{w}\|^2}{2} + \gamma \sum_{i=1}^{N} \xi_i
$$

### 하이퍼파라미터 `γ` (또는 `C`)의 역할

`γ`는 오류에 대한 벌점의 중요도를 조절합니다.

* **`γ = 0`**: 오류에 대한 벌점이 0이 됩니다. 모델은 분류를 완전히 포기하고 오직 마진을 넓히는 데만 집중하여, 결국 `w = 0` (무한한 마진)이 됩니다.
* **`γ → ∞`**: 오류에 대한 벌점이 무한대가 됩니다. 모델은 어떻게든 오류(`Σξᵢ`)를 줄이는 것을 최우선으로 합니다.
    * **데이터가 분리 가능하면**: 오류를 0으로 만드는 것이 가능하므로, **하드 마진 SVM처럼 행동**하며 마진 최대화에 집중합니다.
    * **데이터가 분리 불가능하면**: 오류를 0으로 만들 수 없으므로, 마진을 희생해서라도 **오류를 최소화하는 데에만 집중**합니다.

### 힌지 손실 (Hinge Loss)

위의 목적 함수는 **힌지 손실**을 사용하여 재구성할 수 있습니다.

$$
\min_{\mathbf{w},w_0} \sum_{i=1}^{N} \max\{0, 1 - y^{(i)}(\mathbf{w}^T\mathbf{x}^{(i)} + w_0)\} + \frac{1}{2\gamma} \|\mathbf{w}\|^2
$$

* **힌지 손실**: `L(y, ŷ) = max(0, 1 - yŷ)`
    * **`yŷ ≥ 1` (정답 & 마진 확보)**: Loss = 0. 잘 맞춘 데이터는 무시합니다.
    * **`yŷ < 1` (마진 위반 또는 오분류)**: Loss > 0. 마진을 위반한 정도에 비례하여 선형적으로 벌점을 부과합니다.

---

## 4. 비선형 분류와 커널 트릭 (Kernel Trick)

XOR 문제처럼 선형으로 분리할 수 없는 데이터를 분류하기 위한 기법입니다.

* **핵심 아이디어**: 저차원 데이터를 고차원 공간으로 매핑(`φ`)하여, 고차원에서는 선형(초평면)으로 분리할 수 있도록 만듭니다.
* **커널 '트릭'**: SVM은 계산 시 실제 매핑된 좌표 `φ(x)`가 아닌, 좌표 간의 내적 `φ(xᵢ)ᵀφ(xⱼ)` 값만 필요로 합니다. 커널 함수 `K(xᵢ, xⱼ)`는 이 **내적 값을 고차원으로 데이터를 보내지 않고도 저차원에서 효율적으로 계산**해줍니다.
* **주의점**: 매핑 함수(커널)를 잘못 선택하면 고차원에서 데이터가 겹치거나 더 얽힐 수 있습니다. 따라서 데이터에 맞는 커널과 하이퍼파라미터를 선택하는 것이 중요합니다.

---

## 5. 다중 클래스 분류 (Multiclass Classification)

SVM은 기본적으로 이진 분류기이므로, 3개 이상의 클래스를 분류하기 위해서는 여러 전략이 필요합니다.

### One-vs-Rest (OvR)

* **방식**: K개의 클래스가 있을 때, 각 클래스마다 "해당 클래스 vs 나머지"를 구분하는 K개의 분류기를 만듭니다.
* **문제점**: 각 분류기가 독립적으로 학습되므로, 어떤 데이터가 여러 분류기에 의해 'Positive'로 판정되는 **모호한 영역(ambiguous region)**이 발생할 수 있습니다. (1개 이상의 정답)

### One-vs-One (OvO)

* **방식**: 모든 클래스 쌍에 대해 "클래스 A vs 클래스 B"를 구분하는 `K(K-1)/2`개의 분류기를 만들고, 투표(majority vote)로 최종 클래스를 결정합니다.
* **문제점**: 분류기들의 판정이 "가위바위보"처럼 순환 관계를 이룰 수 있습니다. (`C1 > C2`, `C2 > C3`인데 `C3 > C1`인 경우) 이로 인해 1:1:1 동점 상황이 발생하여 아무 클래스로도 결정되지 않는 **미결정 영역(Gridlock)**이 생길 수 있습니다.
