
# k-최근접 이웃 (k-NN) 핵심 개념 정리

## 1. Large k와 과소적합(Underfitting)

k-NN 알고리즘에서 `k`값은 모델의 복잡도를 결정하는 핵심 하이퍼파라미터입니다. `k`가 너무 크면 모델이 과소적합(Underfitting)될 수 있습니다.

### 🤔 왜 Large `k`가 과소적합을 유발할까?

`k`가 크다는 것은 예측 시 더 넓은 범위의 이웃 데이터를 참고한다는 의미입니다. 이는 다음과 같은 결과를 낳습니다.

1.  **결정 경계의 단순화 (Smoothing the Decision Boundary)**
    더 많은 데이터를 평균내어 예측하므로, 개별 데이터 포인트나 작은 데이터 그룹(노이즈, 이상치 등)이 결정 경계에 미치는 영향이 줄어듭니다. 결과적으로 결정 경계는 매우 부드럽고 단순한 형태로 변하며, 데이터의 복잡한 패턴을 놓치게 됩니다.

2.  **지역적 특성 무시 (Ignoring Local Patterns)**
    예측하려는 지점 주변에 특정 클래스의 작은 군집이 있더라도, `k`가 너무 크면 이 군집의 "의견"은 더 넓은 영역을 차지하는 다수 클래스의 "의견"에 묻혀버립니다. 즉, 모델이 데이터의 중요한 디테일과 지역적 특성을 학습하지 못하게 됩니다.

> **💡 비유: 맛집 추천**
> * **Small `k` (소수 전문가):** 내 주변 3명에게 맛집을 묻는 것과 같습니다. 매우 지역적이고 상세하지만, 한 명의 독특한 취향에 결과가 왜곡될 수 있습니다. **(과적합 경향)**
> * **Large `k` (다수 대중):** 동네 사람 100명에게 설문조사하는 것과 같습니다. 가장 보편적이고 안정적인 추천(예: 프랜차이즈)을 받지만, 숨겨진 맛집(중요 패턴)은 놓치게 됩니다. **(과소적합 경향)**

---

## 2. k값 선택을 위한 경험 법칙 (Rule of Thumb)

최적의 `k`값을 찾기 위한 절대적인 규칙은 없지만, 탐색의 시작점으로 삼기 좋은 유용한 경험 법칙이 있습니다.

### 📝 Rule of Thumb: $k < \sqrt{n}$

-   $n$: 전체 훈련 데이터의 개수(number of training examples)

이 규칙은 모델의 복잡도와 안정성 사이의 균형을 맞추려는 경험적 가이드라인입니다.

### 🧐 왜 제곱근($\sqrt{n}$)을 사용할까?

제곱근 함수는 $n$이 증가할수록 **증가율이 둔화**되는 특징이 있습니다.

-   **$n$이 작을 때:** $\sqrt{n}$은 상대적으로 큰 값이므로, 모델이 노이즈에 과도하게 반응하는 것을 막아 안정성을 높입니다.
-   **$n$이 클 때:** $k$가 $n$에 비례해서 너무 커지는 것을 방지합니다. 덕분에 데이터가 많아져도 모델이 지역적 패턴을 학습할 수 있는 능력을 유지하게 됩니다.

### ⚠️ 중요: 현실적인 사용법

이 규칙은 **최적의 $k$값이 아닌, 탐색을 시작할 합리적인 범위를 제시**하는 것입니다.

**최적의 $k$를 찾는 가장 표준적이고 신뢰도 높은 방법은 항상 교차 검증(Cross-Validation)입니다.**

> **올바른 사용 순서:**
> 1. 내 데이터의 개수 $n$을 확인하고 $\sqrt{n}$을 계산한다.
> 2. $\sqrt{n}$ 주변의 홀수들을 $k$의 후보군으로 설정한다.
> 3. **교차 검증**을 통해 후보군 중에서 가장 성능이 좋은 $k$를 최종적으로 선택한다.
