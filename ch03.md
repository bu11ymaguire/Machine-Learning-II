## 머신러닝 개념 요약

---

### ## 1. 머신러닝의 기본 목표

* **핵심 목표**: 주어진 훈련 데이터 `(x, y)`로부터 입력 `x`와 출력 `y`의 관계를 나타내는 최적의 함수 `f(x)`를 찾는 것.
* **궁극적인 목표**: 학습에 사용되지 않은 **새로운 데이터(unseen data)**에 대해서도 정확하게 예측하는 것. 이를 **일반화(Generalization)**라고 함.
* **오차의 종류**:
    * **일반화 오차 (Generalization Error)**: 모델의 실제 성능. 모든 가능한 데이터에 대한 오차의 기댓값으로, 계산이 불가능.
    * **훈련 오차 (Training Error)**: 훈련 데이터에 대한 오차. 과적합(Overfitting) 때문에 일반화 성능의 나쁜 대리 지표가 될 수 있음.
    * **테스트 오차 (Test Error)**: 훈련에 사용되지 않은 테스트 데이터에 대한 오차. 일반화 오차를 가장 잘 추정하는 **대리 지표(proxy)**로 사용됨.

---

### ## 2. 지도 학습의 주요 유형

* **분류 (Classification)**: 데이터를 정해진 **카테고리**로 나누는 문제. (예: 스팸/정상 메일, 개/고양이 이미지)
* **회귀 (Regression)**: **연속적인 숫자 값**을 예측하는 문제. (예: 집값, 주가, 온도 예측)

---

### ## 3. 학습 원리와 가정

* **학습 메커니즘**: **손실 함수(Loss Function)**를 정의하고, 그 값을 최소화하는 함수 `f`를 찾는 과정.
    * **손실 함수**: 모델의 예측값과 실제 정답이 얼마나 다른지를 나타내는 '벌점' 함수.
    * **경험적 손실 (Empirical Loss)**: 훈련 데이터에 대한 평균 손실. 실제 학습에서는 이 값을 최소화함.
    * **기대 손실 (Expected Loss)**: 모든 데이터에 대한 손실의 기댓값. 우리가 궁극적으로 줄이고자 하는 이론적 목표.
* **I.I.D. 가정**: 훈련 데이터와 테스트 데이터가 **독립적이고(Independently) 동일한 분포(Identically Distributed)**를 따른다는 핵심 가정. 이 가정이 있기에, 훈련 데이터로 학습한 모델이 테스트 데이터에서도 잘 작동할 것이라는 **신뢰의 다리**가 놓임.

---

### ## 4. 특징과 표현 학습

* **전통적 방식 (Handcrafting Features)**: 사람이 직접 사전 지식을 이용해 데이터의 중요한 특징(feature)을 설계하고 추출하여 모델에 입력.
* **현대적 방식 (Representation Learning)**: 모델이 원본 데이터(raw data)로부터 직접 유용한 특징(표현)을 스스로 학습. **딥러닝(Deep Learning)**이 대표적인 표현 학습 방법론.

---

### ## 5. 선형 회귀와 선형대수학

* **모델**: $f(x) = w^T x + b$
* **편향(Bias term, $b$)**: 모델이 원점을 지나야 한다는 제약에서 벗어나게 해주는 **y절편**. 모델의 **유연성**을 높여줌. 수학적 편의를 위해 특징 벡터 `x`에 1을 추가하여 가중치 벡터 `w`에 흡수시킬 수 있음.
* **핵심 원리**: 선형 회귀에서 **평균 제곱 오차(MSE)를 최소화**하는 것은 선형대수학에서 **실제 정답 벡터($y$)**를 **특징 벡터들이 만드는 공간($X$의 열 공간)으로 정사영(Orthogonal Projection)시키는 것**과 수학적, 기하학적으로 완벽히 동일한 문제.
    * 이때 찾는 것은 최적의 투영을 만드는 **가중치 벡터($W$)**.
    * $Y$를 $\hat{Y}$로 변환하는 행렬을 **투영 행렬(Projection Matrix)**이라 함.
