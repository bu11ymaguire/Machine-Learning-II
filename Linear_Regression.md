

---

# 선형 회귀, 최소제곱해, 그리고 투영의 관계 📝

> 머신러닝의 **선형 회귀(Linear Regression)** 모델은 어떻게 데이터에 가장 적합한 선을 찾을까요? 그 해답은 선형대수학의 **최소제곱해(Least Squares Solution)**와 **정사영(Orthogonal Projection)** 개념에 깊이 뿌리내리고 있습니다.
>
> 이 세 가지 개념은 사실상 **같은 문제를 다른 관점에서 바라보는 것**이며, 수학적으로 완벽하게 동일한 결론에 도달합니다.

---

### ## 1. 문제 정의: 두 가지 관점

우리의 목표는 주어진 데이터 (X, y)를 가장 잘 설명하는 가중치 벡터 w를 찾는 것입니다.

#### 🤖 머신러닝: 선형 회귀 (최적화 관점)

선형 회귀는 실제 값(y)과 모델의 예측 값(y_hat = Xw) 사이의 **오차 제곱의 합(Sum of Squared Errors)을 최소화**하는 것을 목표로 합니다. 이를 비용 함수(Cost Function) J(w)로 표현합니다.

J(w) = (1/n) * ||Xw - y||^2

우리의 목표는 J(w)를 최소로 만드는 w를 찾는 것입니다.

#### 📐 선형대수학: 최소제곱해 (방정식 풀이 관점)

이상적으로는 모든 데이터를 완벽하게 만족하는 연립방정식 Xw = y의 해 w를 찾고 싶습니다. 하지만 현실의 데이터에는 오차가 존재하므로 이 방정식을 만족하는 해는 보통 존재하지 않습니다. (즉, 벡터 y가 행렬 X의 **열공간(Column Space)** 밖에 있습니다.)

따라서 우리는 원래의 y와 가장 가까운, 즉 오차 벡터 e = y - Xw의 크기(norm) ||e||를 최소화하는 **근사해(approximate solution)** w_hat을 찾습니다. 이것이 바로 **최소제곱해**입니다.

Minimize ||y - Xw||^2

---

### ## 2. 해법: 정규방정식 (The Normal Equation)

두 가지 관점은 결국 동일한 해법으로 귀결됩니다. 바로 **정규방정식**입니다.

-   **최적화 관점:** 비용 함수 J(w)를 w에 대해 미분하여 그 기울기(gradient)가 0이 되는 지점을 찾으면 정규방정식이 유도됩니다.

    grad_w J(w) = (2/n) * X^T * (Xw - y) = 0

-   **기하학적 관점:** 최소제곱해를 찾으려면 오차 벡터 (y - X*w_hat)가 X의 열공간과 **수직(orthogonal)**이어야 합니다. 이는 오차 벡터가 X의 모든 열벡터와 내적했을 때 0이 된다는 의미이며, 행렬로 표현하면 다음과 같습니다.

    X^T * (y - X*w_hat) = 0

두 방식 모두 아래의 동일한 정규방정식을 유도합니다.

X^T * X * w_hat = X^T * y

그리고 이 식을 w_hat에 대해 풀면 선형 회귀의 해를 한 번에 구할 수 있습니다.

w_hat = (X^T * X)^(-1) * X^T * y

---

### ## 3. 기하학적 해석: 투영 (Projection)

최소제곱해를 찾는 과정은 기하학적으로 실제 값 벡터 y를 모델이 표현할 수 있는 공간(즉, X의 열공간)에 **정사영(Orthogonal Projection)**시키는 것과 같습니다.



-   **예측 벡터 y_hat:** 선형 회귀 모델의 예측 값 벡터 y_hat = X * w_hat은 바로 y를 X의 열공간에 정사영시킨 결과입니다.

-   **투영 행렬 P:** 이 정사영을 수행하는 변환 행렬을 **투영 행렬(Projection Matrix)** P라고 합니다.

    y_hat = X * w_hat = X * ( (X^T * X)^(-1) * X^T * y )

    y_hat = ( X * (X^T * X)^(-1) * X^T ) * y

    따라서 투영 행렬 P는 다음과 같습니다.

    P = X * (X^T * X)^(-1) * X^T

    P는 어떤 벡터 y라도 모델 공간으로 곧바로 투영하여 가장 근사한 예측 값 y_hat을 만들어 줍니다.

---

### ## 4. 요약

| 개념         | 관점   | 목표                                  | 핵심 결과                                |
| :----------- | :----- | :------------------------------------ | :--------------------------------------- |
| **선형 회귀** | 최적화 | 비용 함수 J(w) 최소화                 | 정규방정식을 통해 w 도출                 |
| **최소제곱법** | 대수학 | 근사 방정식 \|\|y - Xw\|\|^2 최소화      | 정규방정식을 통해 w_hat 도출             |
| **투영** | 기하학 | y를 X의 열공간에 정사영               | 투영 행렬 P로 y_hat 도출                 |

결론적으로, 선형 회귀 모델을 학습시키는 것은 데이터 X가 정의하는 모델 공간으로 실제 정답 y를 정사영하여 오차를 최소화하는 예측값 y_hat을 찾는 수학적 과정입니다. 이 깊은 연관성을 이해하면 선형 회귀가 왜, 그리고 어떻게 작동하는지에 대한 탄탄한 이론적 기반을 갖추게 됩니다.
